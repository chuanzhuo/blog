{"title":"用Docker部署大语言模型Llama 3","slug":"用Docker部署大语言模型Llama-3","date":"2024-04-21T08:38:43.000Z","updated":"2025-11-27T12:23:12.894Z","comments":true,"path":"api/articles/用Docker部署大语言模型Llama-3.json","excerpt":" [Figure] AI, 大语言模型","covers":["https://img.carlzeng.com:3/i/2025/11/27/692841a55d698.png","https://img.carlzeng.com:3/i/2025/11/21/6920195f995a9.png","https://img.carlzeng.com:3/i/2025/11/21/692046c34be1e.png","https://www.gstatic.com/youtube/img/watch/yt_favicon_ringo2.png"],"content":"<img class=\"lozad\" data-src=\"https://img.carlzeng.com:3/i/2025/11/27/692841a55d698.png\">\n\n<p>AI, 大语言模型</p>\n<span id=\"more\"></span>\n\n<div> \n<button onclick=\"synthesizeSpeech()\">朗读全文</button>\n</div>\n<audio controls id=\"audioPlayer\">Your browser does not support the audio element.</audio>      \n<script>\n  function synthesizeSpeech() { \n    var inputText = document.getElementsByClassName('post-block')[0].innerText;\n    var voice = \"ZH\";\n    var url = 'https://tts.carlzeng.com:3/speech?text=' + encodeURIComponent(inputText.substring(0,3000)) + '&voice=' + voice;\n    var audioPlayer = document.getElementById('audioPlayer');          \n    audioPlayer.src = url;\n    audioPlayer.load();\n    audioPlayer.play();\n  }\n</script>\n\n\n\n\n<h1 id=\"有什么用\"><a href=\"#有什么用\" class=\"headerlink\" title=\"有什么用\"></a>有什么用</h1><p>体验一下AI目前在CPU上运行的应用; 用来学习AI大语言模型</p>\n<p>本文被停滞了将近1年半时间; 受制于硬件和时间关系; 即便最近跑通了也更尴尬. 只要一请求打一点的运算, CPU马上100%占用. 下一步: 安排英伟达的GPU登场</p>\n<h1 id=\"怎么用\"><a href=\"#怎么用\" class=\"headerlink\" title=\"怎么用\"></a>怎么用</h1><h2 id=\"拉取Ollama镜像命令\"><a href=\"#拉取Ollama镜像命令\" class=\"headerlink\" title=\"*拉取Ollama镜像命令\"></a>*拉取Ollama镜像命令</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker  pull ollama/ollama:latest</span><br></pre></td></tr></table></figure>\n\n<p>然后在bash命令界面,执行<code>ollama run llama2</code>命令，接着等待下载即可,最后出现success,表示下载运行Llama 2模型成功</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec -it ollama-ollama1 bash    </span><br><span class=\"line\">root@ollama-ollama1:/# ollama run llama2                                                                  </span><br><span class=\"line\">pulling manifest                                                                                          </span><br><span class=\"line\">pulling 8934d96d3f08...  89% ▕█████████████████████████████████████     ▏ 3.4 GB/3.8 GB  4.8 MB/s   1m26s</span><br></pre></td></tr></table></figure>\n\n<p>教程：<a href=\"https://github.com/ollama/ollama\">https://github.com/ollama/ollama</a></p>\n<p>Remove a model<br>    ollama rm llama2</p>\n<p>卡死；删除整个container，空间没有得到释放，郁闷。。。</p>\n<p>20240423放弃，因为对CPU的占用太大。</p>\n<h2 id=\"拉取Chatbot-Ollama（这是UI）镜像命令\"><a href=\"#拉取Chatbot-Ollama（这是UI）镜像命令\" class=\"headerlink\" title=\"*拉取Chatbot-Ollama（这是UI）镜像命令\"></a>*拉取Chatbot-Ollama（这是UI）镜像命令</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker  pull ghcr.io/ivanfioravanti/chatbot-ollama:main</span><br></pre></td></tr></table></figure>\n\n<p>然后设置这个UI的一个变量：该变量就是连接我们上面运行Ollama框架服务的地址,我们设置本地地址:</p>\n<p>http:&#x2F;&#x2F;群晖局域网IP:11434</p>\n<h1 id=\"相关内容\"><a href=\"#相关内容\" class=\"headerlink\" title=\"相关内容\"></a>相关内容</h1><iframe style=\"box-shadow: 0px 0px 20px -10px;\" src=\"https://query.carlzeng.com:3/appsearch?q=ai\" frameborder=\"0\" scrolling=\"auto\" width=\"100%\" height=\"500\"></iframe>\n\n<h1 id=\"实现方法\"><a href=\"#实现方法\" class=\"headerlink\" title=\"实现方法\"></a>实现方法</h1><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">version:</span> <span class=\"string\">&quot;3&quot;</span></span><br><span class=\"line\"><span class=\"attr\">services:</span></span><br><span class=\"line\">    <span class=\"attr\">localai:</span></span><br><span class=\"line\">        <span class=\"attr\">tty:</span> <span class=\"literal\">true</span></span><br><span class=\"line\">        <span class=\"attr\">stdin_open:</span> <span class=\"literal\">true</span></span><br><span class=\"line\">        <span class=\"attr\">container_name:</span> <span class=\"string\">local-ai</span></span><br><span class=\"line\">        <span class=\"attr\">ports:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"number\">8105</span><span class=\"string\">:8080</span></span><br><span class=\"line\">        <span class=\"attr\">image:</span> <span class=\"string\">localai/localai:latest-aio-cpu</span></span><br><span class=\"line\">        <span class=\"attr\">volumes:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">~/models:/models:cached</span></span><br></pre></td></tr></table></figure>\n\n<p>运行之前: </p>\n<p><img data-src=\"https://img.carlzeng.com:3/i/2025/11/21/6920195f995a9.png\" alt=\"image-20251121154835867\"></p>\n<p>运行之后: </p>\n<p><img data-src=\"https://img.carlzeng.com:3/i/2025/11/21/692046c34be1e.png\" alt=\"image-20251121190221555\"></p>\n<p>可见CPU直接飙满了~. 还好当没有请求的时候cpu不是一直这样保持爆满的状态</p>\n<p>curl <a href=\"http://192.168.6.117:8105/tts\">http://192.168.6.117:8105/tts</a> -H “Content-Type: application&#x2F;json” -d ‘{<br>  “input”: “Hello world”,<br>  “model”: “tts”<br>}’</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Load model from /models/tts failed:Load model /models/tts failed. File doesn&#x27;t exist&quot;</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"CPU-only\"><a href=\"#CPU-only\" class=\"headerlink\" title=\"CPU only\"></a>CPU only</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">    ollama:</span><br><span class=\"line\">        volumes:</span><br><span class=\"line\">            - ./ollama:/root/.ollama</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">            - 8106:11434</span><br><span class=\"line\">        container_name: ollama</span><br><span class=\"line\">        image: ollama/ollama</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Run-a-model\"><a href=\"#Run-a-model\" class=\"headerlink\" title=\"Run a model\"></a>Run a model</h3><p>Now you can run a model like Llama 2 inside the container.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker <span class=\"built_in\">exec</span> -it ollama ollama run llama2-chinese</span><br></pre></td></tr></table></figure>\n\n\n\n<p>用上一次的webui 关联起来就是</p>\n<p>给ollama弄一个前端UI</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">version:</span> <span class=\"string\">&quot;3&quot;</span></span><br><span class=\"line\"><span class=\"attr\">services:</span></span><br><span class=\"line\">    <span class=\"attr\">open-webui:</span></span><br><span class=\"line\">        <span class=\"attr\">ports:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"number\">8107</span><span class=\"string\">:8080</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"number\">11434</span><span class=\"string\">:11434</span></span><br><span class=\"line\">        <span class=\"attr\">extra_hosts:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">host.docker.internal:host-gateway</span></span><br><span class=\"line\">        <span class=\"attr\">volumes:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">./open-webui-data:/app/backend/data</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">/root/ollama/ollama:/root/.ollama</span></span><br><span class=\"line\">        <span class=\"attr\">container_name:</span> <span class=\"string\">open-webui</span></span><br><span class=\"line\">        <span class=\"attr\">restart:</span> <span class=\"string\">always</span></span><br><span class=\"line\">        <span class=\"attr\">image:</span> <span class=\"string\">ghcr.io/open-webui/open-webui:main</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>        - /root/ollama/ollama:/root/.ollama \n        - 尝试关联本机中的ollama(独立运行的)\n        - \n</code></pre>\n<p><a href=\"https://www.runoob.com/ollama/ollama-open-webui.html\">https://www.runoob.com/ollama/ollama-open-webui.html</a></p>\n<h2 id=\"TTS\"><a href=\"#TTS\" class=\"headerlink\" title=\"TTS\"></a>TTS</h2><p><a href=\"https://www.youtube.com/watch?v=r8r1VFbhh1w\">Coqui TTS Setup via Docker: Voice AI on Linux Mint</a></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu</span><br><span class=\"line\"></span><br><span class=\"line\">python3 TTS/server/server.py --model_name tts_models/en/vctk/vits</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">version:</span> <span class=\"string\">&quot;3&quot;</span></span><br><span class=\"line\"><span class=\"attr\">services:</span></span><br><span class=\"line\">    <span class=\"attr\">ollama:</span></span><br><span class=\"line\">        <span class=\"attr\">volumes:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">./ollama:/root/.ollama</span></span><br><span class=\"line\">        <span class=\"attr\">ports:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"number\">8106</span><span class=\"string\">:11434</span></span><br><span class=\"line\">        <span class=\"attr\">container_name:</span> <span class=\"string\">ollama</span></span><br><span class=\"line\">        <span class=\"attr\">image:</span> <span class=\"string\">ollama/ollama</span></span><br></pre></td></tr></table></figure>\n\n<p><a href=\"http://192.168.6.117:8106/\">http://192.168.6.117:8106/</a></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ollama is running</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Run-a-model-1\"><a href=\"#Run-a-model-1\" class=\"headerlink\" title=\"Run a model\"></a>Run a model</h3><p>Now you can run a model like Llama 2 inside the container.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker <span class=\"built_in\">exec</span> -it ollama ollama run ChatTTS-Ollama</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>StyleTTS - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFV0VE5VbnJvX1k3cWhCdmFPZk1qcWZJWVlpUXxBQ3Jtc0tucEU0OUM1YXRjOXVDQS1MNlRWOTQ5RE5md2NNR0pvNExMdGcyblJBWGgwMks4ZU9LOEdLdUo2MnUyUVZTdlVYUl83VFl5Qm43anpOaF9XSmpHbjQtNkxDTF9Xd1haZ0l5SU9CWGl5TFk1ZkR5dzk3cw&q=https://github.com/yl4579/StyleTTS2?tab=readme-ov-file&v=lPitjhhodaw\">https://github.com/yl4579/StyleTTS2?t...</a> Eleven’s Style TTS - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbVBhS01WTlI2eFNWZ1lhajh3eWxQLUVSQ083Z3xBQ3Jtc0trWnYyVV8zOExUaGExNUVXX2ZmNjcxT3pqMm5FdzVMbXlmR1dvXzVkZDJtLXRqTVdZQUJyWHlaVGd3ZlhwRndfOVdEOTVRVlE5Vjc2SnltV1JqdnVFMUZyazhkV3N0X1dhdWdqSzJ2Y3Y2OXk1bklaSQ&q=https://github.com/IIEleven11/StyleTTS2FineTune&v=lPitjhhodaw\">https://github.com/IIEleven11/StyleTT...</a> </p>\n<p>Coqui TTS - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbllvV1M0U3duWF9teUpBUWE3LVJmUW5rbFlwQXxBQ3Jtc0tud3hOSXlLMUo3Vk9mYzI0YkY5N3pLNnVtSF9lZlJ5UDUzanc5akNmb3hrcnh0YmVzbmdCZl9aSklydGVPUjEzdm40ekVlbU1IWWhXU2o5bWNwN2x2Y2tad2NhdldJUjVyNDluSFp4XzhrTzJJcURQNA&q=https://github.com/coqui-ai/TTS&v=lPitjhhodaw\">https://github.com/coqui-ai/TTS</a> </p>\n<p>Daswers XTTS GUI - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa296V1ZRQnVCNkdUSDNSN1Z5cTFNdk5ZLWthQXxBQ3Jtc0tsXzNGczhVa1hYTTR2ZC0yLWhvZ1ZZVl9PX3JHM1p1Zm5IdU4zcW52V0E1QVJHem4xZzhuQW5fVGgxTExtXzNaYmRBOGRCQlZRamEwRk9mdmxYbFZfUnRCZ1lTUXEwam05UEFaaGpuT2hydkdLRkhnOA&q=https://github.com/daswer123/xtts-finetune-webui&v=lPitjhhodaw\">https://github.com/daswer123/xtts-fin...</a> </p>\n<p>Suno Bark - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2stNnpaWXlNcjhJLW5rWTNkOU1RbllQaE1kZ3xBQ3Jtc0tuYWpQSUc5dER6cFB6WVd3MUE0SXJlenNHNWZHdURuWVI1SmFkQUlsNUM0ak5kbFNHX0F5ekwySVE2d21FZGFscmc1M3Jlcm9ORjlOazhHM3A3QmN6QlYxdi1OTHFHUVpWZ1hLaWlYUm5zclBReUY0aw&q=https://github.com/suno-ai/bark&v=lPitjhhodaw\">https://github.com/suno-ai/bark</a><br>VallE-X - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbEJjSTlsWjREOER0R0FxZWVOTW0xeXhacWJJQXxBQ3Jtc0trUndvNFQ4LUkySzdnZzlHUHQ1WU9EbUdTTy1WRVJlREdMc0R5TmI0LXlncTJNeWdwRTNudGZialYwRDc1cDFHbS1wYk8tVEg3WXJkMDRvSjJGSWxCZnVYdDVpRzllXzcxSUNQRnU0Z28zc3I1UEdWRQ&q=https://github.com/Plachtaa/VALL-E-X&v=lPitjhhodaw\">https://github.com/Plachtaa/VALL-E-X</a> </p>\n<p><del>Tortoise TTS Installation - <a href=\"https://www.youtube.com/watch?v=p31Ax_A5VKA&pp=0gcJCR0AztywvtLA\"> <img data-src=\"https://www.gstatic.com/youtube/img/watch/yt_favicon_ringo2.png\" alt=\"img\"> • Local AI Voice Cloning with Tortoise TTS -…  </a></del> NVIDIA GPUs</p>\n<p>最终的成功案例: </p>\n<p><a href=\"https://github.com/jianchang512/ChatTTS-ui/tree/main\">https://github.com/jianchang512/ChatTTS-ui/tree/main</a></p>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><ol>\n<li><p>拉取项目仓库</p>\n<p>在任意路径下克隆项目，例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/jianchang512/ChatTTS-ui.git chat-tts-ui</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>启动 Runner</p>\n<p>进入到项目目录：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd chat-tts-ui</span><br></pre></td></tr></table></figure>\n\n\n\n<p>启动容器并查看初始化日志：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gpu版本</span><br><span class=\"line\">docker compose -f docker-compose.gpu.yaml up -d </span><br><span class=\"line\"></span><br><span class=\"line\">cpu版本    </span><br><span class=\"line\">docker compose -f docker-compose.cpu.yaml up -d</span><br><span class=\"line\"></span><br><span class=\"line\">docker compose logs -f --no-log-prefix</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>访问 ChatTTS WebUI</p>\n<p><code>启动:[&#39;0.0.0.0&#39;, &#39;9966&#39;]</code>，也即，访问部署设备的 <code>IP:9966</code> 即可，例如：</p>\n<ul>\n<li>本机：<code>http://127.0.0.1:9966</code></li>\n<li>服务器: <code>http://192.168.1.100:9966</code></li>\n</ul>\n</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Attaching to chat-tts-ui                                                                                        </span><br><span class=\"line\">chat-tts-ui  | Starting...                                                                                      </span><br><span class=\"line\">chat-tts-ui  | Traceback (most recent call last):                                                               </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/app.py&quot;, line 24, in                                                        </span><br><span class=\"line\">chat-tts-ui  |     import ChatTTS                                                                               </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/__init__.py&quot;, line 1, in                                            </span><br><span class=\"line\">chat-tts-ui  |     from .core import Chat                                                                       </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/core.py&quot;, line 18, in                                               </span><br><span class=\"line\">chat-tts-ui  |     from .model import DVAE, GPT, gen_logits, Tokenizer                                          </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/model/__init__.py&quot;, line 4, in                                      </span><br><span class=\"line\">chat-tts-ui  |     from .tokenizer import Tokenizer                                                             </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/model/tokenizer.py&quot;, line 20, in                                    </span><br><span class=\"line\">chat-tts-ui  |     class Tokenizer:                                                                             </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/model/tokenizer.py&quot;, line 22, in Tokenizer                                  </span><br><span class=\"line\">chat-tts-ui  |     self, tokenizer_path: torch.serialization.FILE_LIKE, device: torch.device                    </span><br><span class=\"line\">chat-tts-ui  | AttributeError: module &#x27;torch.serialization&#x27; has no attribute &#x27;FILE_LIKE&#x27;                        </span><br><span class=\"line\">chat-tts-ui exited with code 1                                                                                  </span><br><span class=\"line\">chat-tts-ui  | Starting...                                                                                      </span><br><span class=\"line\">  Gracefully stopping... (press Ctrl+C again to force)                                                          </span><br><span class=\"line\">[+] Stopping 1/1                                                                                                </span><br><span class=\"line\"> ✔ Container chat-tts-ui  Stopped</span><br></pre></td></tr></table></figure>\n\n<p>解决办法: <a href=\"https://github.com/2noise/ChatTTS/issues/933\">https://github.com/2noise/ChatTTS/issues/933</a></p>\n<p>修改requirements.txt 中的 torch&#x3D;&#x3D;2.1.0 </p>\n<p>用法1: <a href=\"https://tts1.carlzeng.com:3/\">https://tts1.carlzeng.com:3/</a></p>\n<p>用法2: </p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># API调用代码</span><br><span class=\"line\"></span><br><span class=\"line\">import requests</span><br><span class=\"line\"></span><br><span class=\"line\">res = requests.post(&#x27;https://tts1.carlzeng.com:3/tts&#x27;, data=&#123;</span><br><span class=\"line\">  &quot;text&quot;: &quot;若不懂无需填写&quot;,</span><br><span class=\"line\">  &quot;prompt&quot;: &quot;&quot;,</span><br><span class=\"line\">  &quot;voice&quot;: &quot;3333&quot;,</span><br><span class=\"line\">  &quot;temperature&quot;: 0.3,</span><br><span class=\"line\">  &quot;top_p&quot;: 0.7,</span><br><span class=\"line\">  &quot;top_k&quot;: 20,</span><br><span class=\"line\">  &quot;skip_refine&quot;: 0,</span><br><span class=\"line\">  &quot;custom_voice&quot;: 0</span><br><span class=\"line\">&#125;)</span><br><span class=\"line\">print(res.json())</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n\n\n<h1 id=\"灵感来源\"><a href=\"#灵感来源\" class=\"headerlink\" title=\"灵感来源\"></a>灵感来源</h1><p><a href=\"https://cloud.tencent.com.cn/developer/article/2406985?areaId=106001\">群晖NAS使用Docker部署大语言模型Llama 2结合内网穿透实现公网访问本地GPT聊天服务</a></p>\n<p>docker 部署 Liama</p>\n<p><a href=\"https://github.com/LlamaFamily/Llama-Chinese\">https://github.com/LlamaFamily/Llama-Chinese</a></p>\n","more":"<div> \n<button onclick=\"synthesizeSpeech()\">朗读全文</button>\n</div>\n<audio controls id=\"audioPlayer\">Your browser does not support the audio element.</audio>      \n<script>\n  function synthesizeSpeech() { \n    var inputText = document.getElementsByClassName('post-block')[0].innerText;\n    var voice = \"ZH\";\n    var url = 'https://tts.carlzeng.com:3/speech?text=' + encodeURIComponent(inputText.substring(0,3000)) + '&voice=' + voice;\n    var audioPlayer = document.getElementById('audioPlayer');          \n    audioPlayer.src = url;\n    audioPlayer.load();\n    audioPlayer.play();\n  }\n</script>\n\n\n\n\n<h1 id=\"有什么用\"><a href=\"#有什么用\" class=\"headerlink\" title=\"有什么用\"></a>有什么用</h1><p>体验一下AI目前在CPU上运行的应用; 用来学习AI大语言模型</p>\n<p>本文被停滞了将近1年半时间; 受制于硬件和时间关系; 即便最近跑通了也更尴尬. 只要一请求打一点的运算, CPU马上100%占用. 下一步: 安排英伟达的GPU登场</p>\n<h1 id=\"怎么用\"><a href=\"#怎么用\" class=\"headerlink\" title=\"怎么用\"></a>怎么用</h1><h2 id=\"拉取Ollama镜像命令\"><a href=\"#拉取Ollama镜像命令\" class=\"headerlink\" title=\"*拉取Ollama镜像命令\"></a>*拉取Ollama镜像命令</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker  pull ollama/ollama:latest</span><br></pre></td></tr></table></figure>\n\n<p>然后在bash命令界面,执行<code>ollama run llama2</code>命令，接着等待下载即可,最后出现success,表示下载运行Llama 2模型成功</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec -it ollama-ollama1 bash    </span><br><span class=\"line\">root@ollama-ollama1:/# ollama run llama2                                                                  </span><br><span class=\"line\">pulling manifest                                                                                          </span><br><span class=\"line\">pulling 8934d96d3f08...  89% ▕█████████████████████████████████████     ▏ 3.4 GB/3.8 GB  4.8 MB/s   1m26s</span><br></pre></td></tr></table></figure>\n\n<p>教程：<a href=\"https://github.com/ollama/ollama\">https://github.com/ollama/ollama</a></p>\n<p>Remove a model<br>    ollama rm llama2</p>\n<p>卡死；删除整个container，空间没有得到释放，郁闷。。。</p>\n<p>20240423放弃，因为对CPU的占用太大。</p>\n<h2 id=\"拉取Chatbot-Ollama（这是UI）镜像命令\"><a href=\"#拉取Chatbot-Ollama（这是UI）镜像命令\" class=\"headerlink\" title=\"*拉取Chatbot-Ollama（这是UI）镜像命令\"></a>*拉取Chatbot-Ollama（这是UI）镜像命令</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker  pull ghcr.io/ivanfioravanti/chatbot-ollama:main</span><br></pre></td></tr></table></figure>\n\n<p>然后设置这个UI的一个变量：该变量就是连接我们上面运行Ollama框架服务的地址,我们设置本地地址:</p>\n<p>http:&#x2F;&#x2F;群晖局域网IP:11434</p>\n<h1 id=\"相关内容\"><a href=\"#相关内容\" class=\"headerlink\" title=\"相关内容\"></a>相关内容</h1><iframe style=\"box-shadow: 0px 0px 20px -10px;\" src=\"https://query.carlzeng.com:3/appsearch?q=ai\" frameborder=\"0\" scrolling=\"auto\" width=\"100%\" height=\"500\"></iframe>\n\n<h1 id=\"实现方法\"><a href=\"#实现方法\" class=\"headerlink\" title=\"实现方法\"></a>实现方法</h1><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">version:</span> <span class=\"string\">&quot;3&quot;</span></span><br><span class=\"line\"><span class=\"attr\">services:</span></span><br><span class=\"line\">    <span class=\"attr\">localai:</span></span><br><span class=\"line\">        <span class=\"attr\">tty:</span> <span class=\"literal\">true</span></span><br><span class=\"line\">        <span class=\"attr\">stdin_open:</span> <span class=\"literal\">true</span></span><br><span class=\"line\">        <span class=\"attr\">container_name:</span> <span class=\"string\">local-ai</span></span><br><span class=\"line\">        <span class=\"attr\">ports:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"number\">8105</span><span class=\"string\">:8080</span></span><br><span class=\"line\">        <span class=\"attr\">image:</span> <span class=\"string\">localai/localai:latest-aio-cpu</span></span><br><span class=\"line\">        <span class=\"attr\">volumes:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">~/models:/models:cached</span></span><br></pre></td></tr></table></figure>\n\n<p>运行之前: </p>\n<p><img data-src=\"https://img.carlzeng.com:3/i/2025/11/21/6920195f995a9.png\" alt=\"image-20251121154835867\"></p>\n<p>运行之后: </p>\n<p><img data-src=\"https://img.carlzeng.com:3/i/2025/11/21/692046c34be1e.png\" alt=\"image-20251121190221555\"></p>\n<p>可见CPU直接飙满了~. 还好当没有请求的时候cpu不是一直这样保持爆满的状态</p>\n<p>curl <a href=\"http://192.168.6.117:8105/tts\">http://192.168.6.117:8105/tts</a> -H “Content-Type: application&#x2F;json” -d ‘{<br>  “input”: “Hello world”,<br>  “model”: “tts”<br>}’</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Load model from /models/tts failed:Load model /models/tts failed. File doesn&#x27;t exist&quot;</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"CPU-only\"><a href=\"#CPU-only\" class=\"headerlink\" title=\"CPU only\"></a>CPU only</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &quot;3&quot;</span><br><span class=\"line\">services:</span><br><span class=\"line\">    ollama:</span><br><span class=\"line\">        volumes:</span><br><span class=\"line\">            - ./ollama:/root/.ollama</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">            - 8106:11434</span><br><span class=\"line\">        container_name: ollama</span><br><span class=\"line\">        image: ollama/ollama</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Run-a-model\"><a href=\"#Run-a-model\" class=\"headerlink\" title=\"Run a model\"></a>Run a model</h3><p>Now you can run a model like Llama 2 inside the container.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker <span class=\"built_in\">exec</span> -it ollama ollama run llama2-chinese</span><br></pre></td></tr></table></figure>\n\n\n\n<p>用上一次的webui 关联起来就是</p>\n<p>给ollama弄一个前端UI</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">version:</span> <span class=\"string\">&quot;3&quot;</span></span><br><span class=\"line\"><span class=\"attr\">services:</span></span><br><span class=\"line\">    <span class=\"attr\">open-webui:</span></span><br><span class=\"line\">        <span class=\"attr\">ports:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"number\">8107</span><span class=\"string\">:8080</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"number\">11434</span><span class=\"string\">:11434</span></span><br><span class=\"line\">        <span class=\"attr\">extra_hosts:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">host.docker.internal:host-gateway</span></span><br><span class=\"line\">        <span class=\"attr\">volumes:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">./open-webui-data:/app/backend/data</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">/root/ollama/ollama:/root/.ollama</span></span><br><span class=\"line\">        <span class=\"attr\">container_name:</span> <span class=\"string\">open-webui</span></span><br><span class=\"line\">        <span class=\"attr\">restart:</span> <span class=\"string\">always</span></span><br><span class=\"line\">        <span class=\"attr\">image:</span> <span class=\"string\">ghcr.io/open-webui/open-webui:main</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>        - /root/ollama/ollama:/root/.ollama \n        - 尝试关联本机中的ollama(独立运行的)\n        - \n</code></pre>\n<p><a href=\"https://www.runoob.com/ollama/ollama-open-webui.html\">https://www.runoob.com/ollama/ollama-open-webui.html</a></p>\n<h2 id=\"TTS\"><a href=\"#TTS\" class=\"headerlink\" title=\"TTS\"></a>TTS</h2><p><a href=\"https://www.youtube.com/watch?v=r8r1VFbhh1w\">Coqui TTS Setup via Docker: Voice AI on Linux Mint</a></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu</span><br><span class=\"line\"></span><br><span class=\"line\">python3 TTS/server/server.py --model_name tts_models/en/vctk/vits</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">version:</span> <span class=\"string\">&quot;3&quot;</span></span><br><span class=\"line\"><span class=\"attr\">services:</span></span><br><span class=\"line\">    <span class=\"attr\">ollama:</span></span><br><span class=\"line\">        <span class=\"attr\">volumes:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"string\">./ollama:/root/.ollama</span></span><br><span class=\"line\">        <span class=\"attr\">ports:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"number\">8106</span><span class=\"string\">:11434</span></span><br><span class=\"line\">        <span class=\"attr\">container_name:</span> <span class=\"string\">ollama</span></span><br><span class=\"line\">        <span class=\"attr\">image:</span> <span class=\"string\">ollama/ollama</span></span><br></pre></td></tr></table></figure>\n\n<p><a href=\"http://192.168.6.117:8106/\">http://192.168.6.117:8106/</a></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ollama is running</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Run-a-model-1\"><a href=\"#Run-a-model-1\" class=\"headerlink\" title=\"Run a model\"></a>Run a model</h3><p>Now you can run a model like Llama 2 inside the container.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker <span class=\"built_in\">exec</span> -it ollama ollama run ChatTTS-Ollama</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>StyleTTS - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFV0VE5VbnJvX1k3cWhCdmFPZk1qcWZJWVlpUXxBQ3Jtc0tucEU0OUM1YXRjOXVDQS1MNlRWOTQ5RE5md2NNR0pvNExMdGcyblJBWGgwMks4ZU9LOEdLdUo2MnUyUVZTdlVYUl83VFl5Qm43anpOaF9XSmpHbjQtNkxDTF9Xd1haZ0l5SU9CWGl5TFk1ZkR5dzk3cw&q=https://github.com/yl4579/StyleTTS2?tab=readme-ov-file&v=lPitjhhodaw\">https://github.com/yl4579/StyleTTS2?t...</a> Eleven’s Style TTS - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbVBhS01WTlI2eFNWZ1lhajh3eWxQLUVSQ083Z3xBQ3Jtc0trWnYyVV8zOExUaGExNUVXX2ZmNjcxT3pqMm5FdzVMbXlmR1dvXzVkZDJtLXRqTVdZQUJyWHlaVGd3ZlhwRndfOVdEOTVRVlE5Vjc2SnltV1JqdnVFMUZyazhkV3N0X1dhdWdqSzJ2Y3Y2OXk1bklaSQ&q=https://github.com/IIEleven11/StyleTTS2FineTune&v=lPitjhhodaw\">https://github.com/IIEleven11/StyleTT...</a> </p>\n<p>Coqui TTS - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbllvV1M0U3duWF9teUpBUWE3LVJmUW5rbFlwQXxBQ3Jtc0tud3hOSXlLMUo3Vk9mYzI0YkY5N3pLNnVtSF9lZlJ5UDUzanc5akNmb3hrcnh0YmVzbmdCZl9aSklydGVPUjEzdm40ekVlbU1IWWhXU2o5bWNwN2x2Y2tad2NhdldJUjVyNDluSFp4XzhrTzJJcURQNA&q=https://github.com/coqui-ai/TTS&v=lPitjhhodaw\">https://github.com/coqui-ai/TTS</a> </p>\n<p>Daswers XTTS GUI - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa296V1ZRQnVCNkdUSDNSN1Z5cTFNdk5ZLWthQXxBQ3Jtc0tsXzNGczhVa1hYTTR2ZC0yLWhvZ1ZZVl9PX3JHM1p1Zm5IdU4zcW52V0E1QVJHem4xZzhuQW5fVGgxTExtXzNaYmRBOGRCQlZRamEwRk9mdmxYbFZfUnRCZ1lTUXEwam05UEFaaGpuT2hydkdLRkhnOA&q=https://github.com/daswer123/xtts-finetune-webui&v=lPitjhhodaw\">https://github.com/daswer123/xtts-fin...</a> </p>\n<p>Suno Bark - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2stNnpaWXlNcjhJLW5rWTNkOU1RbllQaE1kZ3xBQ3Jtc0tuYWpQSUc5dER6cFB6WVd3MUE0SXJlenNHNWZHdURuWVI1SmFkQUlsNUM0ak5kbFNHX0F5ekwySVE2d21FZGFscmc1M3Jlcm9ORjlOazhHM3A3QmN6QlYxdi1OTHFHUVpWZ1hLaWlYUm5zclBReUY0aw&q=https://github.com/suno-ai/bark&v=lPitjhhodaw\">https://github.com/suno-ai/bark</a><br>VallE-X - <a href=\"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbEJjSTlsWjREOER0R0FxZWVOTW0xeXhacWJJQXxBQ3Jtc0trUndvNFQ4LUkySzdnZzlHUHQ1WU9EbUdTTy1WRVJlREdMc0R5TmI0LXlncTJNeWdwRTNudGZialYwRDc1cDFHbS1wYk8tVEg3WXJkMDRvSjJGSWxCZnVYdDVpRzllXzcxSUNQRnU0Z28zc3I1UEdWRQ&q=https://github.com/Plachtaa/VALL-E-X&v=lPitjhhodaw\">https://github.com/Plachtaa/VALL-E-X</a> </p>\n<p><del>Tortoise TTS Installation - <a href=\"https://www.youtube.com/watch?v=p31Ax_A5VKA&pp=0gcJCR0AztywvtLA\"> <img data-src=\"https://www.gstatic.com/youtube/img/watch/yt_favicon_ringo2.png\" alt=\"img\"> • Local AI Voice Cloning with Tortoise TTS -…  </a></del> NVIDIA GPUs</p>\n<p>最终的成功案例: </p>\n<p><a href=\"https://github.com/jianchang512/ChatTTS-ui/tree/main\">https://github.com/jianchang512/ChatTTS-ui/tree/main</a></p>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><ol>\n<li><p>拉取项目仓库</p>\n<p>在任意路径下克隆项目，例如：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/jianchang512/ChatTTS-ui.git chat-tts-ui</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>启动 Runner</p>\n<p>进入到项目目录：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd chat-tts-ui</span><br></pre></td></tr></table></figure>\n\n\n\n<p>启动容器并查看初始化日志：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gpu版本</span><br><span class=\"line\">docker compose -f docker-compose.gpu.yaml up -d </span><br><span class=\"line\"></span><br><span class=\"line\">cpu版本    </span><br><span class=\"line\">docker compose -f docker-compose.cpu.yaml up -d</span><br><span class=\"line\"></span><br><span class=\"line\">docker compose logs -f --no-log-prefix</span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>访问 ChatTTS WebUI</p>\n<p><code>启动:[&#39;0.0.0.0&#39;, &#39;9966&#39;]</code>，也即，访问部署设备的 <code>IP:9966</code> 即可，例如：</p>\n<ul>\n<li>本机：<code>http://127.0.0.1:9966</code></li>\n<li>服务器: <code>http://192.168.1.100:9966</code></li>\n</ul>\n</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Attaching to chat-tts-ui                                                                                        </span><br><span class=\"line\">chat-tts-ui  | Starting...                                                                                      </span><br><span class=\"line\">chat-tts-ui  | Traceback (most recent call last):                                                               </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/app.py&quot;, line 24, in                                                        </span><br><span class=\"line\">chat-tts-ui  |     import ChatTTS                                                                               </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/__init__.py&quot;, line 1, in                                            </span><br><span class=\"line\">chat-tts-ui  |     from .core import Chat                                                                       </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/core.py&quot;, line 18, in                                               </span><br><span class=\"line\">chat-tts-ui  |     from .model import DVAE, GPT, gen_logits, Tokenizer                                          </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/model/__init__.py&quot;, line 4, in                                      </span><br><span class=\"line\">chat-tts-ui  |     from .tokenizer import Tokenizer                                                             </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/model/tokenizer.py&quot;, line 20, in                                    </span><br><span class=\"line\">chat-tts-ui  |     class Tokenizer:                                                                             </span><br><span class=\"line\">chat-tts-ui  |   File &quot;/app/ChatTTS/model/tokenizer.py&quot;, line 22, in Tokenizer                                  </span><br><span class=\"line\">chat-tts-ui  |     self, tokenizer_path: torch.serialization.FILE_LIKE, device: torch.device                    </span><br><span class=\"line\">chat-tts-ui  | AttributeError: module &#x27;torch.serialization&#x27; has no attribute &#x27;FILE_LIKE&#x27;                        </span><br><span class=\"line\">chat-tts-ui exited with code 1                                                                                  </span><br><span class=\"line\">chat-tts-ui  | Starting...                                                                                      </span><br><span class=\"line\">  Gracefully stopping... (press Ctrl+C again to force)                                                          </span><br><span class=\"line\">[+] Stopping 1/1                                                                                                </span><br><span class=\"line\"> ✔ Container chat-tts-ui  Stopped</span><br></pre></td></tr></table></figure>\n\n<p>解决办法: <a href=\"https://github.com/2noise/ChatTTS/issues/933\">https://github.com/2noise/ChatTTS/issues/933</a></p>\n<p>修改requirements.txt 中的 torch&#x3D;&#x3D;2.1.0 </p>\n<p>用法1: <a href=\"https://tts1.carlzeng.com:3/\">https://tts1.carlzeng.com:3/</a></p>\n<p>用法2: </p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># API调用代码</span><br><span class=\"line\"></span><br><span class=\"line\">import requests</span><br><span class=\"line\"></span><br><span class=\"line\">res = requests.post(&#x27;https://tts1.carlzeng.com:3/tts&#x27;, data=&#123;</span><br><span class=\"line\">  &quot;text&quot;: &quot;若不懂无需填写&quot;,</span><br><span class=\"line\">  &quot;prompt&quot;: &quot;&quot;,</span><br><span class=\"line\">  &quot;voice&quot;: &quot;3333&quot;,</span><br><span class=\"line\">  &quot;temperature&quot;: 0.3,</span><br><span class=\"line\">  &quot;top_p&quot;: 0.7,</span><br><span class=\"line\">  &quot;top_k&quot;: 20,</span><br><span class=\"line\">  &quot;skip_refine&quot;: 0,</span><br><span class=\"line\">  &quot;custom_voice&quot;: 0</span><br><span class=\"line\">&#125;)</span><br><span class=\"line\">print(res.json())</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n\n\n<h1 id=\"灵感来源\"><a href=\"#灵感来源\" class=\"headerlink\" title=\"灵感来源\"></a>灵感来源</h1><p><a href=\"https://cloud.tencent.com.cn/developer/article/2406985?areaId=106001\">群晖NAS使用Docker部署大语言模型Llama 2结合内网穿透实现公网访问本地GPT聊天服务</a></p>\n<p>docker 部署 Liama</p>\n<p><a href=\"https://github.com/LlamaFamily/Llama-Chinese\">https://github.com/LlamaFamily/Llama-Chinese</a></p>","categories":[{"name":"ollama","path":"api/categories/ollama.json"}],"tags":[{"name":"AI","path":"api/tags/AI.json"},{"name":"Llama 3","path":"api/tags/Llama 3.json"},{"name":"tts","path":"api/tags/tts.json"}]}