{"title":"PVE群晖NAS修复笔记","slug":"PVE群晖NAS修复笔记","date":"2023-12-29T00:51:31.000Z","updated":"2024-07-11T07:37:01.365Z","comments":true,"path":"api/articles/PVE群晖NAS修复笔记.json","excerpt":"调皮的小伙伴把NAS玩坏了，贴出详尽修复过程","covers":["https://www.evernote.com/shard/s122/sh/f5f45ccb-94fe-4531-9f99-ce837554aea3/Y6bPGzuLPbLecEeltpEQTCp16NGYKNDzw4q8hDKFRuDYIXj-Ett5KVKXRQ/deep/0/image.png","https://img.carlzeng.top:3/i/2024/07/11/668f863ae7758.png","https://img.carlzeng.top:3/i/2024/07/11/668f865810e55.png","https://img.carlzeng.top:3/i/2024/07/11/668f86b2d0382.png","https://img.carlzeng.top:3/i/2024/04/17/661f40b956487.png","https://img.carlzeng.top:3/i/2024/04/17/661f40bcd9848.png","https://img.carlzeng.top:3/i/2024/04/17/661f40b088708.png","https://img.carlzeng.top:3/i/2024/04/17/661f42778ca3e.png","https://img.carlzeng.top:3/i/2024/04/17/661f43ddd8091.png","https://img.carlzeng.top:3/i/2024/04/22/66264a39070f7.png"],"content":"<p>调皮的小伙伴把NAS玩坏了，贴出详尽修复过程</p>\n<span id=\"more\"></span>\n\n<img class=\"lozad\" data-src=\"https://www.evernote.com/shard/s122/sh/f5f45ccb-94fe-4531-9f99-ce837554aea3/Y6bPGzuLPbLecEeltpEQTCp16NGYKNDzw4q8hDKFRuDYIXj-Ett5KVKXRQ/deep/0/image.png\">\n\n<h1 id=\"排查修复结果总结\"><a href=\"#排查修复结果总结\" class=\"headerlink\" title=\"排查修复结果总结\"></a>排查修复结果总结</h1><ol>\n<li>MariaDB 修复需要赋值两个映射出来目录的权限，给mysql用户，777的目录权限</li>\n<li>yourls的sql仍然无法修复，只能全部重装后，使用import export插件导入数据，丢失了几周的数据，哎</li>\n<li>删除了pve，debian中的所有陈旧的日志文件，和大的冗余的日志文件。设置了日志文件的新规则。</li>\n<li>进一步了解了pve的磁盘以及分区原理，为下一步重新划分磁盘，分配要更合理一些，迫不及待了….</li>\n</ol>\n<h1 id=\"有什么用\"><a href=\"#有什么用\" class=\"headerlink\" title=\"有什么用\"></a>有什么用</h1><p>排查PVE中NAS的运行错误，控制台错误信息（导致linux无法正常启动）：</p>\n<p>sata boot support on this platform is experimental</p>\n<p>关闭虚拟机，后尝试重启，pve错误：</p>\n<p>WARN: no efidisk configured! Using temporary efivars disk.<br>Warning: unable to close filehandle GEN7208 properly: No space left on device at &#x2F;usr&#x2F;share&#x2F;perl5&#x2F;PVE&#x2F;Tools.pm line 254.<br>TASK ERROR: unable to write ‘&#x2F;tmp&#x2F;105-ovmf.fd.tmp.29425’ - No space left on device</p>\n<h2 id=\"当前的后果（2023-12-29）\"><a href=\"#当前的后果（2023-12-29）\" class=\"headerlink\" title=\"当前的后果（2023.12.29）\"></a>当前的后果（2023.12.29）</h2><p>NAS上面跑的docker也全部挂掉</p>\n<p>​\tBook</p>\n<p>​\temby</p>\n<p>​\taria2</p>\n<p>数据，还有NAS中的数据（群龙无首了）</p>\n<h1 id=\"采取措施\"><a href=\"#采取措施\" class=\"headerlink\" title=\"采取措施\"></a>采取措施</h1><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">qm list </span><br><span class=\"line\">qm stop 105</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>cd &#x2F;<br>du -sh *</p>\n</blockquote>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">102G    var                                                                                                   </span><br></pre></td></tr></table></figure>\n\n<p>发现这个var文件夹占用了102G的空间，继续往里面找找</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">100G    lib       </span><br><span class=\"line\">root@lgkdz:/var/lib/vz/images# du -sh *</span><br><span class=\"line\">26G     100</span><br><span class=\"line\">50G     101</span><br><span class=\"line\">656M    102</span><br><span class=\"line\">21G     105</span><br><span class=\"line\"></span><br><span class=\"line\">#排序</span><br><span class=\"line\">du -s /usr/share/* | sort -nr</span><br></pre></td></tr></table></figure>\n\n<p>进debian ssh，tasksel 卸载桌面。</p>\n<p>磁盘依旧99.99%</p>\n<p>发现可能是Debian的磁盘占用不停侵蚀占用128G的SSD：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4.4G    usr                                                                                                   </span><br><span class=\"line\">18G     var                                                                                                   </span><br><span class=\"line\">5.8G    www </span><br></pre></td></tr></table></figure>\n\n<p>var从上次排查（2023年12月15？;目前运行39天，当时运行24天；2周前）的12G，2周涨了6G空间出来。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@Debian11:/var/lib/docker# du -sh *                                                                       </span><br><span class=\"line\">108K    buildkit                                                                                              </span><br><span class=\"line\">1.3G    containers                                                                                            </span><br><span class=\"line\">4.0K    engine-id                                                                                             </span><br><span class=\"line\">27M     image                                                                                                 </span><br><span class=\"line\">244K    network                                                                                               </span><br><span class=\"line\">15G     overlay2                                                                                              </span><br><span class=\"line\">16K     plugins                                                                                               </span><br><span class=\"line\">4.0K    runtimes                                                                                              </span><br><span class=\"line\">4.0K    swarm                                                                                                 </span><br><span class=\"line\">4.0K    tmp                                                                                                   </span><br><span class=\"line\">900K    volumes</span><br></pre></td></tr></table></figure>\n\n<p>清理docker:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; docker system df                                                      </span><br><span class=\"line\">TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE                                                     </span><br><span class=\"line\">Images          21        18        6.589GB   153.2MB (2%)                                                    </span><br><span class=\"line\">Containers      18        17        483.5MB   555.6kB (0%)                                                    </span><br><span class=\"line\">Local Volumes   0         0         0B        0B                                                              </span><br><span class=\"line\">Build Cache     0         0         0B        0B         </span><br><span class=\"line\"></span><br><span class=\"line\">&gt; docker system prune -a</span><br><span class=\"line\">y</span><br><span class=\"line\">Total reclaimed space: 392MB</span><br><span class=\"line\"></span><br><span class=\"line\">&gt; docker system df</span><br><span class=\"line\">TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE</span><br><span class=\"line\">Images          17        17        6.197GB   7.335MB (0%)</span><br><span class=\"line\">Containers      17        17        482.9MB   0B (0%)</span><br><span class=\"line\">Local Volumes   0         0         0B        0B</span><br><span class=\"line\">Build Cache     0         0         0B        0B</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart  docker   </span><br></pre></td></tr></table></figure>\n\n<p>仍然是：99.99% (108.18 GiB的108.20 GiB)</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">PVE的情况</span></span><br><span class=\"line\">root@lgkdz:/# systemctl status pveproxy.service                                                                                  </span><br><span class=\"line\">● pveproxy.service - PVE API Proxy Server                                                                                        </span><br><span class=\"line\">     Loaded: loaded (/lib/systemd/system/pveproxy.service; enabled; preset: enabled)                                             </span><br><span class=\"line\">     Active: active (running) since Sun 2023-11-19 19:25:04 CST; 1 month 9 days ago                                              </span><br><span class=\"line\">    Process: 989 ExecStartPre=/usr/bin/pvecm updatecerts --silent (code=exited, status=0/SUCCESS)                                </span><br><span class=\"line\">    Process: 991 ExecStart=/usr/bin/pveproxy start (code=exited, status=0/SUCCESS)                                               </span><br><span class=\"line\">    Process: 56738 ExecReload=/usr/bin/pveproxy restart (code=exited, status=0/SUCCESS)                                          </span><br><span class=\"line\">   Main PID: 993 (pveproxy)                                                                                                      </span><br><span class=\"line\">      Tasks: 4                                                                                                                   </span><br><span class=\"line\">     Memory: 162.3M                                                                                                              </span><br><span class=\"line\">        CPU: 2h 15min 3.182s                                                                                                     </span><br><span class=\"line\">     CGroup: /system.slice/pveproxy.service                                                                                      </span><br><span class=\"line\">             ├─  993 pveproxy                                                                                                    </span><br><span class=\"line\">             ├─12296 &quot;pveproxy worker&quot;                                                                                           </span><br><span class=\"line\">             ├─12300 &quot;pveproxy worker&quot;                                                                                           </span><br><span class=\"line\">             └─12302 &quot;pveproxy worker&quot;                                                                                           </span><br><span class=\"line\">                                                                                                                                 </span><br><span class=\"line\">Dec 29 10:17:46 lgkdz pveproxy[993]: worker 12296 started                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[12280]: worker exit                                                                               </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: worker 12280 finished                                                                       </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: starting 1 worker(s)                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: worker 12300 started                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: worker 12295 finished                                                                       </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: starting 1 worker(s)                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: worker 12302 started                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[12300]: Warning: unable to close filehandle GEN5 properly: No space left on device at /usr/share/p</span><br><span class=\"line\">erl5/PVE/APIServer/AnyEvent.pm line 1901.                                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[12300]: error writing access log </span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<hr>\n<p>加载下来看看，Kingchuxing里面的500G的数据情况：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mount /dev/sda5 /mnt/sda5</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"清理-管理Linux日志\"><a href=\"#清理-管理Linux日志\" class=\"headerlink\" title=\"清理+管理Linux日志\"></a>清理+管理Linux日志</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rm -rf /log/*.gz  </span><br><span class=\"line\">rm -rf /var/log/*.1</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">journalctl --disk-usage       # 查看占用的磁盘                                                                          </span><br><span class=\"line\">Archived and active journals take up 2.5G in the file system.   </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"> </span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">设置占用的磁盘空间，日志量大于这些后自动删除旧的</span></span><br><span class=\"line\">journalctl --vacuum-size=512M </span><br><span class=\"line\"></span><br><span class=\"line\">Vacuuming done, freed 2.0G of archived journals from /var/log/journal/2afbdd1662c14f99a11ce27fcda8ab85.</span><br><span class=\"line\">Vacuuming done, freed 0B of archived journals from /run/log/journal.</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">2d之前的自动删除</span></span><br><span class=\"line\">journalctl --vacuum-time=2d </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">这一顿清理日志以后，硬盘空间：</span></span><br><span class=\"line\">98.33% (106.39 GiB的108.20 GiB)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>在此尝试启动NAS</p>\n<p>WARN: no efidisk configured! Using temporary efivars disk.<br>TASK WARNINGS: 1</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Debian的日志清理，维护</span><br><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\">journalctl --disk-usage       <span class=\"comment\"># 查看占用的磁盘</span></span></span><br><span class=\"line\">Archived and active journals take up 104.0M in the file system.     </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>1、find查找根下大于800M的文件</p>\n<p>find &#x2F; -size +800M -exec ls -lh {} ;</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># find / -size +800M -exec ls -lh &#123;&#125; \\;</span></span>                                                                   </span><br><span class=\"line\">-r-------- 1 root root 128T Nov 19 19:24 /proc/kcore                                                                         </span><br><span class=\"line\">find: ‘/proc/3193/task/3251/fd/34’: No such file or directory                                                                </span><br><span class=\"line\">find: ‘/proc/3193/task/3251/fd/35’: No such file or directory                                                                </span><br><span class=\"line\">find: ‘/proc/14759’: No such file or directory                                                                               </span><br><span class=\"line\">find: ‘/proc/14779’: No such file or directory                                                                               </span><br><span class=\"line\">find: ‘/proc/14780’: No such file or directory                                                                               </span><br><span class=\"line\">find: ‘/proc/14781/task/14781/fd/5’: No such file or directory                                                               </span><br><span class=\"line\">find: ‘/proc/14781/task/14781/fdinfo/5’: No such file or directory                                                           </span><br><span class=\"line\">find: ‘/proc/14781/fd/6’: No such file or directory                                                                          </span><br><span class=\"line\">find: ‘/proc/14781/fdinfo/6’: No such file or directory                                                                      </span><br><span class=\"line\">-rw-r--r-- 1 root root 1.3G Oct 14 20:50 /var/lib/vz/dump/vzdump-lxc-101-2023_10_14-20_48_21.tar.zst                         </span><br><span class=\"line\">-rw-r----- 1 root root 51G Dec 25 09:46 /var/lib/vz/images/100/vm-100-disk-0.qcow2                                           </span><br><span class=\"line\">-rw-r----- 1 root root 11G Dec 29 13:34 /var/lib/vz/images/102/vm-102-disk-0.qcow2                                           </span><br><span class=\"line\">-rw-r----- 1 root root 101G Dec 29 13:34 /var/lib/vz/images/105/vm-105-disk-2.qcow2                                          </span><br><span class=\"line\">-rw-r----- 1 root root 50G Dec 29 13:34 /var/lib/vz/images/101/vm-101-disk-0.raw                                             </span><br><span class=\"line\">-rw------- 1 root root 4.6G Nov  4 10:52 /core </span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\">root@Debian11:~<span class=\"comment\"># find / -size +800M -exec ls -lh &#123;&#125; \\;</span></span></span><br><span class=\"line\">-rw-r----- 1 root root 1.2G Dec 29 13:43 /var/lib/docker/containers/a611cae746aa6c4b1e3bda308a7935180b79e0f684a75791910430989</span><br><span class=\"line\">1e2c979/a611cae746aa6c4b1e3bda308a7935180b79e0f684a757919104309891e2c979-json.log                                            </span><br><span class=\"line\">-r-------- 1 root root 128T Nov 19 19:26 /proc/kcore </span><br><span class=\"line\">-r-------- 1 root root 128T Dec 17 09:32 /dev/.lxc/proc/kcore</span><br><span class=\"line\"></span><br><span class=\"line\">检查异常大小的log文件</span><br><span class=\"line\">cd /var/lib/docker/containers/a611cae746aa6c4b1e3bda308a7935180b79e0f684a757919104309891e2c979</span><br></pre></td></tr></table></figure>\n\n<p>我认为这个a611cae746aa6c4b1e3bda308a7935180b79e0f684a757919104309891e2c979 是frp的docker，尝试删除这个1.2G的log文件！</p>\n<p>直接rm；没有发现任何异常；怎么会生成这么大的log文件？？</p>\n<p>4:28PM &gt; </p>\n<p>分析两个磁盘的6个分区里面数据占用情况<br>        使用df -h 命令查看文件系统及空间使用情况</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># df -h</span></span>                                                                                                        </span><br><span class=\"line\">Filesystem            Size  Used Avail Use% Mounted on                                                                            </span><br><span class=\"line\">udev                  7.7G     0  7.7G   0% /dev                                                                                  </span><br><span class=\"line\">tmpfs                 1.6G  864K  1.6G   1% /run                                                                                  </span><br><span class=\"line\">/dev/mapper/pve-root  109G  107G     0 100% /                                                                                     </span><br><span class=\"line\">tmpfs                 7.8G   43M  7.7G   1% /dev/shm                                                                              </span><br><span class=\"line\">tmpfs                 5.0M     0  5.0M   0% /run/lock                                                                             </span><br><span class=\"line\">/dev/sdb2            1022M  352K 1022M   1% /boot/efi                                                                             </span><br><span class=\"line\">/dev/fuse             128M   16K  128M   1% /etc/pve                                                                              </span><br><span class=\"line\">tmpfs                 1.6G     0  1.6G   0% /run/user/0   </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">也可用 <span class=\"built_in\">df</span> -T 查看文件系统的Type</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\"><span class=\"built_in\">df</span> -T</span></span><br><span class=\"line\">Filesystem           Type     1K-blocks      Used Available Use% Mounted on</span><br><span class=\"line\">udev                 devtmpfs   8066408         0   8066408   0% /dev</span><br><span class=\"line\">tmpfs                tmpfs      1620188       864   1619324   1% /run</span><br><span class=\"line\">/dev/mapper/pve-root ext4     113455880 111702412         0 100% /</span><br><span class=\"line\">tmpfs                tmpfs      8100928     43680   8057248   1% /dev/shm</span><br><span class=\"line\">tmpfs                tmpfs         5120         0      5120   0% /run/lock</span><br><span class=\"line\">/dev/sdb2            vfat       1046508       352   1046156   1% /boot/efi</span><br><span class=\"line\">/dev/fuse            fuse        131072        16    131056   1% /etc/pve</span><br><span class=\"line\">tmpfs                tmpfs      1620184         0   1620184   0% /run/user/0</span><br><span class=\"line\"></span><br><span class=\"line\"> /dev/mapper/pve-root 就是pve卷组里的一个逻辑卷pve</span><br><span class=\"line\"><span class=\"meta prompt_\"> </span></span><br><span class=\"line\"><span class=\"meta prompt_\"> &gt; </span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># pvdisplay</span></span></span><br><span class=\"line\">  --- Physical volume ---</span><br><span class=\"line\">  PV Name               /dev/sdb3</span><br><span class=\"line\">  VG Name               pve</span><br><span class=\"line\">  PV Size               118.24 GiB / not usable &lt;3.32 MiB</span><br><span class=\"line\">  Allocatable           yes (but full)</span><br><span class=\"line\">  PE Size               4.00 MiB</span><br><span class=\"line\">  Total PE              30269</span><br><span class=\"line\">  Free PE               0</span><br><span class=\"line\">  Allocated PE          30269</span><br><span class=\"line\">  PV UUID               jEzPvE-ELri-mvlq-5Jpi-s96g-a44F-SWWM4N</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"meta prompt_\">  &gt; </span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># vgdisplay</span></span></span><br><span class=\"line\">  --- Volume group ---</span><br><span class=\"line\">  VG Name               pve</span><br><span class=\"line\">  System ID             </span><br><span class=\"line\">  Format                lvm2</span><br><span class=\"line\">  Metadata Areas        1</span><br><span class=\"line\">  Metadata Sequence No  9</span><br><span class=\"line\">  VG Access             read/write</span><br><span class=\"line\">  VG Status             resizable</span><br><span class=\"line\">  MAX LV                0</span><br><span class=\"line\">  Cur LV                2</span><br><span class=\"line\">  Open LV               2</span><br><span class=\"line\">  Max PV                0</span><br><span class=\"line\">  Cur PV                1</span><br><span class=\"line\">  Act PV                1</span><br><span class=\"line\">  VG Size               &lt;118.24 GiB</span><br><span class=\"line\">  PE Size               4.00 MiB</span><br><span class=\"line\">  Total PE              30269</span><br><span class=\"line\">  Alloc PE / Size       30269 / &lt;118.24 GiB</span><br><span class=\"line\">  Free  PE / Size       0 / 0   </span><br><span class=\"line\">  VG UUID               aBqMlz-dH1H-PEif-LGT5-khnl-oEXf-sMtWTc</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"meta prompt_\">  &gt; </span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># lvdisplay</span></span></span><br><span class=\"line\">  --- Logical volume ---</span><br><span class=\"line\">  LV Path                /dev/pve/swap</span><br><span class=\"line\">  LV Name                swap</span><br><span class=\"line\">  VG Name                pve</span><br><span class=\"line\">  LV UUID                Wl0zSQ-Rlkj-4TLc-yyuM-Ntg1-1T27-QK3KOg</span><br><span class=\"line\">  LV Write Access        read/write</span><br><span class=\"line\">  LV Creation host, time proxmox, 2023-07-01 20:13:17 +0800</span><br><span class=\"line\">  LV Status              available</span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">open                 2</span></span><br><span class=\"line\">  LV Size                8.00 GiB</span><br><span class=\"line\">  Current LE             2048</span><br><span class=\"line\">  Segments               1</span><br><span class=\"line\">  Allocation             inherit</span><br><span class=\"line\">  Read ahead sectors     auto</span><br><span class=\"line\">  - currently set to     256</span><br><span class=\"line\">  Block device           253:0</span><br><span class=\"line\">   </span><br><span class=\"line\">  --- Logical volume ---</span><br><span class=\"line\">  LV Path                /dev/pve/root</span><br><span class=\"line\">  LV Name                root</span><br><span class=\"line\">  VG Name                pve</span><br><span class=\"line\">  LV UUID                dFYnFo-1PQw-3qUM-sR9V-2eqf-BKn8-yTASwe</span><br><span class=\"line\">  LV Write Access        read/write</span><br><span class=\"line\">  LV Creation host, time proxmox, 2023-07-01 20:13:17 +0800</span><br><span class=\"line\">  LV Status              available</span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">open                 1</span></span><br><span class=\"line\">  LV Size                &lt;110.24 GiB</span><br><span class=\"line\">  Current LE             28221</span><br><span class=\"line\">  Segments               1</span><br><span class=\"line\">  Allocation             inherit</span><br><span class=\"line\">  Read ahead sectors     auto</span><br><span class=\"line\">  - currently set to     256</span><br><span class=\"line\">  Block device           253:1</span><br></pre></td></tr></table></figure>\n\n\n\n<p>lsblk 查看所有存在的<em>磁盘</em>及<em>分区</em>（不管使用挂载是否）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@lgkdz:/var/log# lsblk                                                                                    </span><br><span class=\"line\">NAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS                                                            </span><br><span class=\"line\">loop0          7:0    0    50G  0 loop                                                                        </span><br><span class=\"line\">sda            8:0    0 476.9G  0 disk                                                                        </span><br><span class=\"line\">├─sda1         8:1    0     8G  0 part                                                                        </span><br><span class=\"line\">├─sda2         8:2    0     2G  0 part                                                                        </span><br><span class=\"line\">├─sda3         8:3    0     1K  0 part                                                                        </span><br><span class=\"line\">└─sda5         8:5    0 466.7G  0 part                                                                        </span><br><span class=\"line\">sdb            8:16   0 119.2G  0 disk                                                                        </span><br><span class=\"line\">├─sdb1         8:17   0  1007K  0 part                                                                        </span><br><span class=\"line\">├─sdb2         8:18   0     1G  0 part /boot/efi                                                              </span><br><span class=\"line\">└─sdb3         8:19   0 118.2G  0 part                                                                        </span><br><span class=\"line\">  ├─pve-swap 253:0    0     8G  0 lvm  [SWAP]                                                                 </span><br><span class=\"line\">  └─pve-root 253:1    0 110.2G  0 lvm  / </span><br></pre></td></tr></table></figure>\n\n<p>您可以通过清理占用磁盘空间较大的文件或目录、扩容磁盘或新购磁盘等几种方式来解决磁盘分区空间使用率达到100%的问题。具体操作步骤如下：</p>\n<p>没办法，杀掉Windows</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\">scp root@192.168.6.2:/var/lib/vz/images/100/vm-100-disk-0.qcow2 .</span>      </span><br><span class=\"line\">root@192.168.6.2&#x27;s password:                                                                                  </span><br><span class=\"line\">vm-100-disk-0.qcow2                                                         100%   50GB  45.1MB/s   18:55 </span><br></pre></td></tr></table></figure>\n\n<p>非正常地退出Matomo和yourls，导致数据库启动出错：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40+00:00 [Note] [Entrypoint]: Switching to dedicated user &#x27;mysql&#x27;               </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40+00:00 [Note] [Entrypoint]: Entrypoint script for MariaDB Server 1:11.2.2+mari</span><br><span class=\"line\">a~ubu2204 started.                                                                                            </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40+00:00 [Note] [Entrypoint]: Initializing database files                       </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40 0 [Warning] Can&#x27;t create test file &#x27;/var/lib/mysql/6be765c4a185.lower-test&#x27; (</span><br><span class=\"line\">Errcode: 13 &quot;Permission denied&quot;)                                                                              </span><br><span class=\"line\">Matomo-DB  | /usr/sbin/mariadbd: Can&#x27;t change dir to &#x27;/var/lib/mysql/&#x27; (Errcode: 13 &quot;Permission denied&quot;)      </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40 0 [ERROR] Aborting                                                           </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | Installation of system tables failed!  Examine the logs in                                       </span><br><span class=\"line\">Matomo-DB  | /var/lib/mysql/ for more information.                                                            </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | The problem could be conflicting information in an external                                      </span><br><span class=\"line\">Matomo-DB  | my.cnf files. You can ignore these by doing:                                                     </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  |     shell&gt; /usr/bin/mariadb-install-db --defaults-file=~/.my.cnf                                 </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | You can also try to start the mariadbd daemon with:                                              </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  |     shell&gt; /usr/sbin/mariadbd --skip-grant-tables --general-log &amp;                                </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | and use the command line tool /usr/bin/mariadb                                                   </span><br><span class=\"line\">Matomo-DB  | to connect to the mysql database and look at the grant tables:                                   </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  |     shell&gt; /usr/bin/mariadb -u root mysql                                                        </span><br><span class=\"line\">Matomo-DB  |     MariaDB&gt; show tables;                                                                        </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | Try &#x27;/usr/sbin/mariadbd --help&#x27; if you have problems with paths.  Using                          </span><br><span class=\"line\">Matomo-DB  | --general-log gives you a log in /var/lib/mysql/ that may be helpful.                            </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | The latest information about mariadb-install-db is available at                                  </span><br><span class=\"line\">Matomo-DB  | https://mariadb.com/kb/en/installing-system-tables-mysql_install_db                              </span><br><span class=\"line\">Matomo-DB  | You can find the latest source at https://downloads.mariadb.org and                              </span><br><span class=\"line\">Matomo-DB  | the maria-discuss email list at https://launchpad.net/~maria-discuss                             </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | Please check all of the above before submitting a bug report                                     </span><br><span class=\"line\">Matomo-DB  | at https://mariadb.org/jira                                                                      </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB exited with code 1             </span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2023-12-29  2:43:11 0 [Note] InnoDB: IO Error: 5during write of 16384 bytes, for file ./matomodb/matomo_log_visit.ibd(16), returned 0</span><br></pre></td></tr></table></figure>\n\n\n\n<p>​\t</p>\n<h3 id=\"Docker-运行-Mysql-容器后报错：-ERROR-–initialize-specified-but-the-data-directory-has-files-in-it-Abort\"><a href=\"#Docker-运行-Mysql-容器后报错：-ERROR-–initialize-specified-but-the-data-directory-has-files-in-it-Abort\" class=\"headerlink\" title=\"Docker - 运行 Mysql 容器后报错：[ERROR] –initialize specified but the data directory has files in it. Abort\"></a>Docker - 运行 Mysql 容器后报错：[ERROR] –initialize specified but the data directory has files in it. Abort</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2023-12-29 11:14:38+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.44-1.el7 started.</span><br><span class=\"line\">2023-12-29 11:14:39+00:00 [Note] [Entrypoint]: Switching to dedicated user &#x27;mysql&#x27;</span><br><span class=\"line\">2023-12-29 11:14:39+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.44-1.el7 started.</span><br><span class=\"line\">2023-12-29 11:14:39+00:00 [Note] [Entrypoint]: Initializing database files</span><br><span class=\"line\">2023-12-29T11:14:39.612526Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).</span><br><span class=\"line\">2023-12-29T11:14:39.615112Z 0 [ERROR] --initialize specified but the data directory has files in it. Aborting.</span><br><span class=\"line\">2023-12-29T11:14:39.615174Z 0 [ERROR] Aborting</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"拯救MaraDB；\"><a href=\"#拯救MaraDB；\" class=\"headerlink\" title=\"拯救MaraDB；\"></a>拯救MaraDB；</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">2023-12-29 13:00:07+00:00 [Note] [Entrypoint]: Switching to dedicated user &#x27;mysql&#x27;                                   </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 13:00:07+00:00 [Note] [Entrypoint]: Entrypoint script for MariaDB Server 1:11.2.2+maria~ubu2204 started.  </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 13:00:07+00:00 [Note] [Entrypoint]: Initializing database files                  </span><br></pre></td></tr></table></figure>\n\n<p>原来是docker-compose 运行maradb需要的权限 给mysql用户分配 执行权限！！</p>\n<p>成功启动maradb！！next：无法运行matomo程序<br>        You don’t have permission to access this resource.Server unable to read htaccess file, denying access to be safe</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Matomo     | [Fri Dec 29 21:28:33.351828 2023] [core:crit] [pid 53] (13)Permission denied: [client 172.21.0.1:5230] AH00529: /var/</span><br><span class=\"line\">www/html/.htaccess pcfg_openfile: unable to check htaccess file, ensure it is readable and that &#x27;/var/www/html/&#x27; is executable, re</span><br><span class=\"line\">ferer: https://query.carlzeng.top:3/</span><br></pre></td></tr></table></figure>\n\n<p>unable to check htaccess file, ensure it is readable and that ‘&#x2F;var&#x2F;www&#x2F;html&#x2F;‘ is executable</p>\n<p>给予777的文件夹权限：matomo</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/07/11/668f863ae7758.png\" alt=\"image-20240711151356442\"></p>\n<p>Maradb修复成功</p>\n<p>Mysql重建成功，丢失数据：lost data: 12.3 - 12.28 之间的数据不知道怎么找回来。。。</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/07/11/668f865810e55.png\" alt=\"image-20240711151426311\"></p>\n<p>修复权限以后显示：</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/07/11/668f86b2d0382.png\" alt=\"image-20240711151556999\"></p>\n<p>到目录&#x2F;www&#x2F;server&#x2F;panel&#x2F;data&#x2F;compose&#x2F;yourls_service&#x2F;template下运行 重新运行 docker-compose up -d</p>\n<p>在PVE中发现磁盘没有被正常加载，这时需要</p>\n<p>mount &#x2F;dev&#x2F;sdd2 &#x2F;mnt&#x2F;usbToshiBa</p>\n<h2 id=\"PVE8-0如何配置和限制日志生成\"><a href=\"#PVE8-0如何配置和限制日志生成\" class=\"headerlink\" title=\"PVE8.0如何配置和限制日志生成\"></a>PVE8.0如何配置和限制日志生成</h2><p>You can limit it by editing “#SystemMaxUse&#x3D;” to something like “SystemMaxUse&#x3D;100M” in “&#x2F;etc&#x2F;systemd&#x2F;journald.conf” and then do a:</p>\n<p> systemctl restart systemd-journald</p>\n<p>下一步就是清理Debian系统的磁盘占用</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@lgkdz:/var/lib/vz# du -s /var/lib/vz/images/* | sort -nr                                                   </span><br><span class=\"line\">51647072        /var/lib/vz/images/101                                                                          </span><br><span class=\"line\">25876720        /var/lib/vz/images/105   </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Debian-清理apt缓存文件\"><a href=\"#Debian-清理apt缓存文件\" class=\"headerlink\" title=\"Debian 清理apt缓存文件\"></a>Debian 清理apt缓存文件</h2><h3 id=\"How-to-clear-the-APT-cache-and-delete-everything-from-var-cache-apt-archives\"><a href=\"#How-to-clear-the-APT-cache-and-delete-everything-from-var-cache-apt-archives\" class=\"headerlink\" title=\"How to clear the APT cache and delete everything from &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;\"></a>How to clear the APT cache and delete everything from &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;</h3><p>The clean command clears out the local repository of retrieved package files. It removes everything but the lock file from &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F; and &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;partial&#x2F;. The syntax is:<br><code>sudo apt clean</code><br>OR<br><code>sudo apt-get clean</code></p>\n<h3 id=\"Delete-all-useless-files-from-the-APT-cache\"><a href=\"#Delete-all-useless-files-from-the-APT-cache\" class=\"headerlink\" title=\"Delete all useless files from the APT cache\"></a>Delete all useless files from the APT cache</h3><p>The syntax is as follows to delete &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;:<br><code>sudo apt autoclean</code><br>OR<br><code>sudo apt-get autoclean</code></p>\n<p>docker system prune</p>\n<h2 id=\"如何设置docker每天定时重启\"><a href=\"#如何设置docker每天定时重启\" class=\"headerlink\" title=\"如何设置docker每天定时重启\"></a>如何设置docker每天定时重启</h2><p>NAS中耗磁盘的docker设置重启</p>\n<p>echo “0 3 * * * root docker restart embyserver” &gt;&gt; &#x2F;etc&#x2F;cron.d&#x2F;my-cron</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;0 3 * * * root docker restart embyserver&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;11 3 * * * root docker restart portainer&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;16 3 * * * root docker restart it-tools&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;21 3 * * * root docker restart espeakbox&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;26 3 * * * root docker restart book-searcher&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;31 3 * * * root docker restart aria2-webui&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;36 3 * * * root docker restart alistaria2-webui&quot; &gt;&gt; /etc/cron.d/my-cron</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>20230109 添加了更多的重启</p>\n<p>继续观察，是否按时重启了这些docker容器？</p>\n<p>答：确实已经是每天定时重启了</p>\n</blockquote>\n<h2 id=\"Debian中耗磁盘的docker设置重启\"><a href=\"#Debian中耗磁盘的docker设置重启\" class=\"headerlink\" title=\"Debian中耗磁盘的docker设置重启\"></a>Debian中耗磁盘的docker设置重启</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">At 02:00am, only on Monday</span></span><br><span class=\"line\">echo &quot;0 2 * * 1 root docker restart iptvchecker-website-1&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;0 3 * * * root docker restart frps&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;10 3 * * * root docker restart chatgpt-next-web&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;15 3 * * * root docker restart hideipnetwork&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;20 3 * * * root docker restart mmPlayer&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;25 3 * * * root docker restart hysteria&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<p>如何改变 &#x2F;dev&#x2F;sda5的大小？</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@lgkdz:~# df -hT                                                                                </span><br><span class=\"line\">Filesystem           Type      Size  Used Avail Use% Mounted on                                     </span><br><span class=\"line\">udev                 devtmpfs  7.7G     0  7.7G   0% /dev                                           </span><br><span class=\"line\">tmpfs                tmpfs     1.6G  880K  1.6G   1% /run                                           </span><br><span class=\"line\">/dev/mapper/pve-root ext4      109G   85G   19G  82% /                                              </span><br><span class=\"line\">tmpfs                tmpfs     7.8G   46M  7.7G   1% /dev/shm                                       </span><br><span class=\"line\">tmpfs                tmpfs     5.0M     0  5.0M   0% /run/lock                                      </span><br><span class=\"line\">/dev/sdb2            vfat     1022M  352K 1022M   1% /boot/efi                                      </span><br><span class=\"line\">/dev/fuse            fuse      128M   16K  128M   1% /etc/pve                                       </span><br><span class=\"line\">tmpfs                tmpfs     1.6G     0  1.6G   0% /run/user/0   </span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"PVE挂载新硬盘\"><a href=\"#PVE挂载新硬盘\" class=\"headerlink\" title=\"PVE挂载新硬盘\"></a>PVE挂载新硬盘</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ls /dev/disk/by-id</span><br><span class=\"line\"></span><br><span class=\"line\">qm set 105 -sata2 /dev/disk/by-id/ata-Kingchuxing_512GB_2023050801426</span><br><span class=\"line\">qm set 105 -sata3 /dev/disk/by-id/usb-TOSHIBA_External_USB_3.0_20140612002491C-0:0</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">这是挂载到特定的虚拟机中去的命令；</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">我需要挂载给pve本身！</span></span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fdisk /dev/sdc   </span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">格式化</span><br><span class=\"line\">mkfs.ext4 /dev/sdc2</span><br><span class=\"line\"></span><br><span class=\"line\">/dev/sdc2 contains a hfsplus file system labelled &#x27;Time Machine Backups&#x27;</span><br><span class=\"line\">Proceed anyway? (y,N) y</span><br><span class=\"line\">Creating filesystem with 244106668 4k blocks and 61030400 inodes</span><br><span class=\"line\">Filesystem UUID: 3991ce4f-dcec-4b28-9370-3ebb751c1912</span><br><span class=\"line\">Superblock backups stored on blocks: </span><br><span class=\"line\">        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, </span><br><span class=\"line\">        4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, </span><br><span class=\"line\">        102400000, 214990848</span><br><span class=\"line\"></span><br><span class=\"line\">Allocating group tables: done                            </span><br><span class=\"line\">Writing inode tables: done                            </span><br><span class=\"line\">Creating journal (262144 blocks): </span><br><span class=\"line\">done</span><br><span class=\"line\">Writing superblocks and filesystem accounting information: done     </span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mount /dev/sdc2 /mnt/usbToshiBa1T</span><br></pre></td></tr></table></figure>\n\n<p>等待测试PVE的显卡直通情况，确保，开机的画面是pve的，这样如果这个命令是错误的（导致无法开机）</p>\n<p>echo &#x2F;dev&#x2F;sdc2 &#x2F;mnt&#x2F;usbToshiBa1T ext4 defaults 0 0 &gt;&gt; &#x2F;etc&#x2F;fstab</p>\n<p>就还知道补救的步骤</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如果这里你操作错误，可能会导致 PVE 无法启动，需要在启动时候接上显示器，进入修复模式 repair filesystem ，直接输入 root 密码即可进入</span><br><span class=\"line\"></span><br><span class=\"line\">因为此时 / 目录是只读模式，进行修改 /etc/fstab 时，提示无法保存（只读），这时需要将 / 目录重新挂载为可读写模式 ，用命令</span><br><span class=\"line\"></span><br><span class=\"line\">mount -o remount,rw,auto /</span><br><span class=\"line\">然后再对 /etc/fstab 进行修改就可以了。重启后系统正常启动。</span><br><span class=\"line\"></span><br><span class=\"line\">之后重启 PVE 即可</span><br><span class=\"line\"></span><br><span class=\"line\">reboot</span><br></pre></td></tr></table></figure>\n\n\n\n<p>这个可以看到已经mount成功了，新磁盘可以使用了</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\"><span class=\"built_in\">df</span> -hT</span>                                                                  </span><br><span class=\"line\">Filesystem           Type      Size  Used Avail Use% Mounted on                                       </span><br><span class=\"line\">udev                 devtmpfs  7.7G     0  7.7G   0% /dev                                             </span><br><span class=\"line\">tmpfs                tmpfs     1.6G  904K  1.6G   1% /run                                             </span><br><span class=\"line\">/dev/mapper/pve-root ext4      109G   85G   19G  82% /                                                </span><br><span class=\"line\">tmpfs                tmpfs     7.8G   46M  7.7G   1% /dev/shm                                         </span><br><span class=\"line\">tmpfs                tmpfs     5.0M     0  5.0M   0% /run/lock                                        </span><br><span class=\"line\">/dev/sdb2            vfat     1022M  352K 1022M   1% /boot/efi                                        </span><br><span class=\"line\">/dev/fuse            fuse      128M   16K  128M   1% /etc/pve                                         </span><br><span class=\"line\">tmpfs                tmpfs     1.6G     0  1.6G   0% /run/user/0                                      </span><br><span class=\"line\">/dev/sdc2            ext4      916G   12K  869G   1% /mnt/usbToshiBa1T    </span><br></pre></td></tr></table></figure>\n\n\n\n<p>本章节参考：</p>\n<p><a href=\"https://www.moewah.com/archives/2546.html\">Proxmox VE（PVE）如何添加多块硬盘</a></p>\n<p><a href=\"https://www.moewah.com/archives/2558.html\">Linux&#x2F;VPS开机启动自动挂载分区的方法</a></p>\n<p><a href=\"https://anuoua.github.io/2019/03/04/Linux%E6%97%A0%E6%8D%9F%E8%B0%83%E6%95%B4%E5%88%86%E5%8C%BA%E5%A4%A7%E5%B0%8F/\">Linux无损调整分区大小</a></p>\n<p><a href=\"https://www.lategege.com/?p=538\">Proxmox VE(PVE)添加硬盘详解</a></p>\n<p>第二天，sdc消失，变成了sdd.</p>\n<p>直接重新拔插USB，还是识别成sdd。删除昨天的sdc2 mount.</p>\n<p>使用pve的UI，点击磁盘》&#x2F;dev&#x2F;sdd2，点 ‘擦除磁盘’，使用率由昨天格式化的ext4 变成了 否。</p>\n<p>umount &#x2F;mnt&#x2F;usbToshiBa1T</p>\n<p>在UI中（目录：磁盘》LVM），新建Volume Group。</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f40b956487.png\" alt=\"image-20240417111839114\"></p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f40bcd9848.png\" alt=\"image-20240417111903877\"></p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f40b088708.png\" alt=\"image-20240417112326350\"></p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f42778ca3e.png\" alt=\"image-20240417113101380\"></p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f43ddd8091.png\" alt=\"image-20240417113659986\"></p>\n<p>top, ps查了半天，还是在pve里面的重启指令生效了 :-)</p>\n<h2 id=\"群晖中清理新硬盘容量\"><a href=\"#群晖中清理新硬盘容量\" class=\"headerlink\" title=\"群晖中清理新硬盘容量\"></a>群晖中清理新硬盘容量</h2><p>日常使用中有手动的把不需要的文件都删除，可是依然发现磁盘的使用率一路表现直到75% 然后 82%（报警）。</p>\n<p>今天发现（其实以前清理过），需要群晖中删除的文件都会默认被挪动到‘回收站’；直到我手动地在该硬盘的根目录，删除‘#recovery’文件夹。磁盘使用率恢复成36%，高兴了 :-)</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/22/66264a39070f7.png\" alt=\"image-20240422192957075\"></p>\n<h2 id=\"分配PVE新的USB磁盘给Debian\"><a href=\"#分配PVE新的USB磁盘给Debian\" class=\"headerlink\" title=\"分配PVE新的USB磁盘给Debian\"></a>分配PVE新的USB磁盘给Debian</h2><p>Debian是以docker的形式挂载在PVE上的，点击pve UI中的Debian11 &gt; ‘资源’ &gt; 添加‘挂载点’</p>\n<p>分配了USB ToShiBa 的硬盘中的 400G给Debian。</p>\n<p>然后备份Debian和Openwrt到这个USB ToShiBa 的硬盘400G空间中，然后都结束以后，PVE又显示磁盘的状态：unknown</p>\n<p>点击磁盘后选择‘备份’显示错误：</p>\n<p>mkdir &#x2F;mnt&#x2F;usbToshiBa&#x2F;&#x2F;dump: Input&#x2F;output error at &#x2F;usr&#x2F;share&#x2F;perl5&#x2F;PVE&#x2F;Storage&#x2F;Plugin.pm line 1389. (500)</p>\n<p>重新拔插了USB的ToshiBa移动硬盘；依然显示状态：unknown</p>\n<p>修复办法：</p>\n<p>​\t切换到PVE的Shell执行</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mount /dev/sdc2 /mnt/usbToshiBa</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">20240423/CZ 尝试通过修改/etc/fstab来获取自动加载，不知道下次会不会自动加载？</span></span><br><span class=\"line\">echo /dev/sdc2 /mnt/usbToshiBa ext4 defaults 0 0 &gt;&gt; /etc/fstab</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"相关内容\"><a href=\"#相关内容\" class=\"headerlink\" title=\"相关内容\"></a>相关内容</h1><iframe style=\"box-shadow: 0px 0px 20px -10px;\" src=\"https://query.carlzeng.top:3/appsearch?q=nas\" frameborder=\"0\" scrolling=\"auto\" width=\"100%\" height=\"500\"></iframe>\n\n<h1 id=\"实现方法\"><a href=\"#实现方法\" class=\"headerlink\" title=\"实现方法\"></a>实现方法</h1><p>都是些小爱心小公益的项目，没必要花时间攻击哦，沉淀共享，格局放大！～</p>\n","more":"<img class=\"lozad\" data-src=\"https://www.evernote.com/shard/s122/sh/f5f45ccb-94fe-4531-9f99-ce837554aea3/Y6bPGzuLPbLecEeltpEQTCp16NGYKNDzw4q8hDKFRuDYIXj-Ett5KVKXRQ/deep/0/image.png\">\n\n<h1 id=\"排查修复结果总结\"><a href=\"#排查修复结果总结\" class=\"headerlink\" title=\"排查修复结果总结\"></a>排查修复结果总结</h1><ol>\n<li>MariaDB 修复需要赋值两个映射出来目录的权限，给mysql用户，777的目录权限</li>\n<li>yourls的sql仍然无法修复，只能全部重装后，使用import export插件导入数据，丢失了几周的数据，哎</li>\n<li>删除了pve，debian中的所有陈旧的日志文件，和大的冗余的日志文件。设置了日志文件的新规则。</li>\n<li>进一步了解了pve的磁盘以及分区原理，为下一步重新划分磁盘，分配要更合理一些，迫不及待了….</li>\n</ol>\n<h1 id=\"有什么用\"><a href=\"#有什么用\" class=\"headerlink\" title=\"有什么用\"></a>有什么用</h1><p>排查PVE中NAS的运行错误，控制台错误信息（导致linux无法正常启动）：</p>\n<p>sata boot support on this platform is experimental</p>\n<p>关闭虚拟机，后尝试重启，pve错误：</p>\n<p>WARN: no efidisk configured! Using temporary efivars disk.<br>Warning: unable to close filehandle GEN7208 properly: No space left on device at &#x2F;usr&#x2F;share&#x2F;perl5&#x2F;PVE&#x2F;Tools.pm line 254.<br>TASK ERROR: unable to write ‘&#x2F;tmp&#x2F;105-ovmf.fd.tmp.29425’ - No space left on device</p>\n<h2 id=\"当前的后果（2023-12-29）\"><a href=\"#当前的后果（2023-12-29）\" class=\"headerlink\" title=\"当前的后果（2023.12.29）\"></a>当前的后果（2023.12.29）</h2><p>NAS上面跑的docker也全部挂掉</p>\n<p>​\tBook</p>\n<p>​\temby</p>\n<p>​\taria2</p>\n<p>数据，还有NAS中的数据（群龙无首了）</p>\n<h1 id=\"采取措施\"><a href=\"#采取措施\" class=\"headerlink\" title=\"采取措施\"></a>采取措施</h1><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">qm list </span><br><span class=\"line\">qm stop 105</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>cd &#x2F;<br>du -sh *</p>\n</blockquote>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">102G    var                                                                                                   </span><br></pre></td></tr></table></figure>\n\n<p>发现这个var文件夹占用了102G的空间，继续往里面找找</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">100G    lib       </span><br><span class=\"line\">root@lgkdz:/var/lib/vz/images# du -sh *</span><br><span class=\"line\">26G     100</span><br><span class=\"line\">50G     101</span><br><span class=\"line\">656M    102</span><br><span class=\"line\">21G     105</span><br><span class=\"line\"></span><br><span class=\"line\">#排序</span><br><span class=\"line\">du -s /usr/share/* | sort -nr</span><br></pre></td></tr></table></figure>\n\n<p>进debian ssh，tasksel 卸载桌面。</p>\n<p>磁盘依旧99.99%</p>\n<p>发现可能是Debian的磁盘占用不停侵蚀占用128G的SSD：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4.4G    usr                                                                                                   </span><br><span class=\"line\">18G     var                                                                                                   </span><br><span class=\"line\">5.8G    www </span><br></pre></td></tr></table></figure>\n\n<p>var从上次排查（2023年12月15？;目前运行39天，当时运行24天；2周前）的12G，2周涨了6G空间出来。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@Debian11:/var/lib/docker# du -sh *                                                                       </span><br><span class=\"line\">108K    buildkit                                                                                              </span><br><span class=\"line\">1.3G    containers                                                                                            </span><br><span class=\"line\">4.0K    engine-id                                                                                             </span><br><span class=\"line\">27M     image                                                                                                 </span><br><span class=\"line\">244K    network                                                                                               </span><br><span class=\"line\">15G     overlay2                                                                                              </span><br><span class=\"line\">16K     plugins                                                                                               </span><br><span class=\"line\">4.0K    runtimes                                                                                              </span><br><span class=\"line\">4.0K    swarm                                                                                                 </span><br><span class=\"line\">4.0K    tmp                                                                                                   </span><br><span class=\"line\">900K    volumes</span><br></pre></td></tr></table></figure>\n\n<p>清理docker:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; docker system df                                                      </span><br><span class=\"line\">TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE                                                     </span><br><span class=\"line\">Images          21        18        6.589GB   153.2MB (2%)                                                    </span><br><span class=\"line\">Containers      18        17        483.5MB   555.6kB (0%)                                                    </span><br><span class=\"line\">Local Volumes   0         0         0B        0B                                                              </span><br><span class=\"line\">Build Cache     0         0         0B        0B         </span><br><span class=\"line\"></span><br><span class=\"line\">&gt; docker system prune -a</span><br><span class=\"line\">y</span><br><span class=\"line\">Total reclaimed space: 392MB</span><br><span class=\"line\"></span><br><span class=\"line\">&gt; docker system df</span><br><span class=\"line\">TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE</span><br><span class=\"line\">Images          17        17        6.197GB   7.335MB (0%)</span><br><span class=\"line\">Containers      17        17        482.9MB   0B (0%)</span><br><span class=\"line\">Local Volumes   0         0         0B        0B</span><br><span class=\"line\">Build Cache     0         0         0B        0B</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart  docker   </span><br></pre></td></tr></table></figure>\n\n<p>仍然是：99.99% (108.18 GiB的108.20 GiB)</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">PVE的情况</span></span><br><span class=\"line\">root@lgkdz:/# systemctl status pveproxy.service                                                                                  </span><br><span class=\"line\">● pveproxy.service - PVE API Proxy Server                                                                                        </span><br><span class=\"line\">     Loaded: loaded (/lib/systemd/system/pveproxy.service; enabled; preset: enabled)                                             </span><br><span class=\"line\">     Active: active (running) since Sun 2023-11-19 19:25:04 CST; 1 month 9 days ago                                              </span><br><span class=\"line\">    Process: 989 ExecStartPre=/usr/bin/pvecm updatecerts --silent (code=exited, status=0/SUCCESS)                                </span><br><span class=\"line\">    Process: 991 ExecStart=/usr/bin/pveproxy start (code=exited, status=0/SUCCESS)                                               </span><br><span class=\"line\">    Process: 56738 ExecReload=/usr/bin/pveproxy restart (code=exited, status=0/SUCCESS)                                          </span><br><span class=\"line\">   Main PID: 993 (pveproxy)                                                                                                      </span><br><span class=\"line\">      Tasks: 4                                                                                                                   </span><br><span class=\"line\">     Memory: 162.3M                                                                                                              </span><br><span class=\"line\">        CPU: 2h 15min 3.182s                                                                                                     </span><br><span class=\"line\">     CGroup: /system.slice/pveproxy.service                                                                                      </span><br><span class=\"line\">             ├─  993 pveproxy                                                                                                    </span><br><span class=\"line\">             ├─12296 &quot;pveproxy worker&quot;                                                                                           </span><br><span class=\"line\">             ├─12300 &quot;pveproxy worker&quot;                                                                                           </span><br><span class=\"line\">             └─12302 &quot;pveproxy worker&quot;                                                                                           </span><br><span class=\"line\">                                                                                                                                 </span><br><span class=\"line\">Dec 29 10:17:46 lgkdz pveproxy[993]: worker 12296 started                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[12280]: worker exit                                                                               </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: worker 12280 finished                                                                       </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: starting 1 worker(s)                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: worker 12300 started                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: worker 12295 finished                                                                       </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: starting 1 worker(s)                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[993]: worker 12302 started                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[12300]: Warning: unable to close filehandle GEN5 properly: No space left on device at /usr/share/p</span><br><span class=\"line\">erl5/PVE/APIServer/AnyEvent.pm line 1901.                                                                                        </span><br><span class=\"line\">Dec 29 10:17:49 lgkdz pveproxy[12300]: error writing access log </span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<hr>\n<p>加载下来看看，Kingchuxing里面的500G的数据情况：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mount /dev/sda5 /mnt/sda5</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"清理-管理Linux日志\"><a href=\"#清理-管理Linux日志\" class=\"headerlink\" title=\"清理+管理Linux日志\"></a>清理+管理Linux日志</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rm -rf /log/*.gz  </span><br><span class=\"line\">rm -rf /var/log/*.1</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">journalctl --disk-usage       # 查看占用的磁盘                                                                          </span><br><span class=\"line\">Archived and active journals take up 2.5G in the file system.   </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"> </span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">设置占用的磁盘空间，日志量大于这些后自动删除旧的</span></span><br><span class=\"line\">journalctl --vacuum-size=512M </span><br><span class=\"line\"></span><br><span class=\"line\">Vacuuming done, freed 2.0G of archived journals from /var/log/journal/2afbdd1662c14f99a11ce27fcda8ab85.</span><br><span class=\"line\">Vacuuming done, freed 0B of archived journals from /run/log/journal.</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">2d之前的自动删除</span></span><br><span class=\"line\">journalctl --vacuum-time=2d </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">这一顿清理日志以后，硬盘空间：</span></span><br><span class=\"line\">98.33% (106.39 GiB的108.20 GiB)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>在此尝试启动NAS</p>\n<p>WARN: no efidisk configured! Using temporary efivars disk.<br>TASK WARNINGS: 1</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Debian的日志清理，维护</span><br><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\">journalctl --disk-usage       <span class=\"comment\"># 查看占用的磁盘</span></span></span><br><span class=\"line\">Archived and active journals take up 104.0M in the file system.     </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>1、find查找根下大于800M的文件</p>\n<p>find &#x2F; -size +800M -exec ls -lh {} ;</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># find / -size +800M -exec ls -lh &#123;&#125; \\;</span></span>                                                                   </span><br><span class=\"line\">-r-------- 1 root root 128T Nov 19 19:24 /proc/kcore                                                                         </span><br><span class=\"line\">find: ‘/proc/3193/task/3251/fd/34’: No such file or directory                                                                </span><br><span class=\"line\">find: ‘/proc/3193/task/3251/fd/35’: No such file or directory                                                                </span><br><span class=\"line\">find: ‘/proc/14759’: No such file or directory                                                                               </span><br><span class=\"line\">find: ‘/proc/14779’: No such file or directory                                                                               </span><br><span class=\"line\">find: ‘/proc/14780’: No such file or directory                                                                               </span><br><span class=\"line\">find: ‘/proc/14781/task/14781/fd/5’: No such file or directory                                                               </span><br><span class=\"line\">find: ‘/proc/14781/task/14781/fdinfo/5’: No such file or directory                                                           </span><br><span class=\"line\">find: ‘/proc/14781/fd/6’: No such file or directory                                                                          </span><br><span class=\"line\">find: ‘/proc/14781/fdinfo/6’: No such file or directory                                                                      </span><br><span class=\"line\">-rw-r--r-- 1 root root 1.3G Oct 14 20:50 /var/lib/vz/dump/vzdump-lxc-101-2023_10_14-20_48_21.tar.zst                         </span><br><span class=\"line\">-rw-r----- 1 root root 51G Dec 25 09:46 /var/lib/vz/images/100/vm-100-disk-0.qcow2                                           </span><br><span class=\"line\">-rw-r----- 1 root root 11G Dec 29 13:34 /var/lib/vz/images/102/vm-102-disk-0.qcow2                                           </span><br><span class=\"line\">-rw-r----- 1 root root 101G Dec 29 13:34 /var/lib/vz/images/105/vm-105-disk-2.qcow2                                          </span><br><span class=\"line\">-rw-r----- 1 root root 50G Dec 29 13:34 /var/lib/vz/images/101/vm-101-disk-0.raw                                             </span><br><span class=\"line\">-rw------- 1 root root 4.6G Nov  4 10:52 /core </span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\">root@Debian11:~<span class=\"comment\"># find / -size +800M -exec ls -lh &#123;&#125; \\;</span></span></span><br><span class=\"line\">-rw-r----- 1 root root 1.2G Dec 29 13:43 /var/lib/docker/containers/a611cae746aa6c4b1e3bda308a7935180b79e0f684a75791910430989</span><br><span class=\"line\">1e2c979/a611cae746aa6c4b1e3bda308a7935180b79e0f684a757919104309891e2c979-json.log                                            </span><br><span class=\"line\">-r-------- 1 root root 128T Nov 19 19:26 /proc/kcore </span><br><span class=\"line\">-r-------- 1 root root 128T Dec 17 09:32 /dev/.lxc/proc/kcore</span><br><span class=\"line\"></span><br><span class=\"line\">检查异常大小的log文件</span><br><span class=\"line\">cd /var/lib/docker/containers/a611cae746aa6c4b1e3bda308a7935180b79e0f684a757919104309891e2c979</span><br></pre></td></tr></table></figure>\n\n<p>我认为这个a611cae746aa6c4b1e3bda308a7935180b79e0f684a757919104309891e2c979 是frp的docker，尝试删除这个1.2G的log文件！</p>\n<p>直接rm；没有发现任何异常；怎么会生成这么大的log文件？？</p>\n<p>4:28PM &gt; </p>\n<p>分析两个磁盘的6个分区里面数据占用情况<br>        使用df -h 命令查看文件系统及空间使用情况</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># df -h</span></span>                                                                                                        </span><br><span class=\"line\">Filesystem            Size  Used Avail Use% Mounted on                                                                            </span><br><span class=\"line\">udev                  7.7G     0  7.7G   0% /dev                                                                                  </span><br><span class=\"line\">tmpfs                 1.6G  864K  1.6G   1% /run                                                                                  </span><br><span class=\"line\">/dev/mapper/pve-root  109G  107G     0 100% /                                                                                     </span><br><span class=\"line\">tmpfs                 7.8G   43M  7.7G   1% /dev/shm                                                                              </span><br><span class=\"line\">tmpfs                 5.0M     0  5.0M   0% /run/lock                                                                             </span><br><span class=\"line\">/dev/sdb2            1022M  352K 1022M   1% /boot/efi                                                                             </span><br><span class=\"line\">/dev/fuse             128M   16K  128M   1% /etc/pve                                                                              </span><br><span class=\"line\">tmpfs                 1.6G     0  1.6G   0% /run/user/0   </span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">也可用 <span class=\"built_in\">df</span> -T 查看文件系统的Type</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\"><span class=\"built_in\">df</span> -T</span></span><br><span class=\"line\">Filesystem           Type     1K-blocks      Used Available Use% Mounted on</span><br><span class=\"line\">udev                 devtmpfs   8066408         0   8066408   0% /dev</span><br><span class=\"line\">tmpfs                tmpfs      1620188       864   1619324   1% /run</span><br><span class=\"line\">/dev/mapper/pve-root ext4     113455880 111702412         0 100% /</span><br><span class=\"line\">tmpfs                tmpfs      8100928     43680   8057248   1% /dev/shm</span><br><span class=\"line\">tmpfs                tmpfs         5120         0      5120   0% /run/lock</span><br><span class=\"line\">/dev/sdb2            vfat       1046508       352   1046156   1% /boot/efi</span><br><span class=\"line\">/dev/fuse            fuse        131072        16    131056   1% /etc/pve</span><br><span class=\"line\">tmpfs                tmpfs      1620184         0   1620184   0% /run/user/0</span><br><span class=\"line\"></span><br><span class=\"line\"> /dev/mapper/pve-root 就是pve卷组里的一个逻辑卷pve</span><br><span class=\"line\"><span class=\"meta prompt_\"> </span></span><br><span class=\"line\"><span class=\"meta prompt_\"> &gt; </span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># pvdisplay</span></span></span><br><span class=\"line\">  --- Physical volume ---</span><br><span class=\"line\">  PV Name               /dev/sdb3</span><br><span class=\"line\">  VG Name               pve</span><br><span class=\"line\">  PV Size               118.24 GiB / not usable &lt;3.32 MiB</span><br><span class=\"line\">  Allocatable           yes (but full)</span><br><span class=\"line\">  PE Size               4.00 MiB</span><br><span class=\"line\">  Total PE              30269</span><br><span class=\"line\">  Free PE               0</span><br><span class=\"line\">  Allocated PE          30269</span><br><span class=\"line\">  PV UUID               jEzPvE-ELri-mvlq-5Jpi-s96g-a44F-SWWM4N</span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"meta prompt_\">  &gt; </span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># vgdisplay</span></span></span><br><span class=\"line\">  --- Volume group ---</span><br><span class=\"line\">  VG Name               pve</span><br><span class=\"line\">  System ID             </span><br><span class=\"line\">  Format                lvm2</span><br><span class=\"line\">  Metadata Areas        1</span><br><span class=\"line\">  Metadata Sequence No  9</span><br><span class=\"line\">  VG Access             read/write</span><br><span class=\"line\">  VG Status             resizable</span><br><span class=\"line\">  MAX LV                0</span><br><span class=\"line\">  Cur LV                2</span><br><span class=\"line\">  Open LV               2</span><br><span class=\"line\">  Max PV                0</span><br><span class=\"line\">  Cur PV                1</span><br><span class=\"line\">  Act PV                1</span><br><span class=\"line\">  VG Size               &lt;118.24 GiB</span><br><span class=\"line\">  PE Size               4.00 MiB</span><br><span class=\"line\">  Total PE              30269</span><br><span class=\"line\">  Alloc PE / Size       30269 / &lt;118.24 GiB</span><br><span class=\"line\">  Free  PE / Size       0 / 0   </span><br><span class=\"line\">  VG UUID               aBqMlz-dH1H-PEif-LGT5-khnl-oEXf-sMtWTc</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"meta prompt_\">  &gt; </span><span class=\"language-bash\">root@lgkdz:/var/log<span class=\"comment\"># lvdisplay</span></span></span><br><span class=\"line\">  --- Logical volume ---</span><br><span class=\"line\">  LV Path                /dev/pve/swap</span><br><span class=\"line\">  LV Name                swap</span><br><span class=\"line\">  VG Name                pve</span><br><span class=\"line\">  LV UUID                Wl0zSQ-Rlkj-4TLc-yyuM-Ntg1-1T27-QK3KOg</span><br><span class=\"line\">  LV Write Access        read/write</span><br><span class=\"line\">  LV Creation host, time proxmox, 2023-07-01 20:13:17 +0800</span><br><span class=\"line\">  LV Status              available</span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">open                 2</span></span><br><span class=\"line\">  LV Size                8.00 GiB</span><br><span class=\"line\">  Current LE             2048</span><br><span class=\"line\">  Segments               1</span><br><span class=\"line\">  Allocation             inherit</span><br><span class=\"line\">  Read ahead sectors     auto</span><br><span class=\"line\">  - currently set to     256</span><br><span class=\"line\">  Block device           253:0</span><br><span class=\"line\">   </span><br><span class=\"line\">  --- Logical volume ---</span><br><span class=\"line\">  LV Path                /dev/pve/root</span><br><span class=\"line\">  LV Name                root</span><br><span class=\"line\">  VG Name                pve</span><br><span class=\"line\">  LV UUID                dFYnFo-1PQw-3qUM-sR9V-2eqf-BKn8-yTASwe</span><br><span class=\"line\">  LV Write Access        read/write</span><br><span class=\"line\">  LV Creation host, time proxmox, 2023-07-01 20:13:17 +0800</span><br><span class=\"line\">  LV Status              available</span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">open                 1</span></span><br><span class=\"line\">  LV Size                &lt;110.24 GiB</span><br><span class=\"line\">  Current LE             28221</span><br><span class=\"line\">  Segments               1</span><br><span class=\"line\">  Allocation             inherit</span><br><span class=\"line\">  Read ahead sectors     auto</span><br><span class=\"line\">  - currently set to     256</span><br><span class=\"line\">  Block device           253:1</span><br></pre></td></tr></table></figure>\n\n\n\n<p>lsblk 查看所有存在的<em>磁盘</em>及<em>分区</em>（不管使用挂载是否）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@lgkdz:/var/log# lsblk                                                                                    </span><br><span class=\"line\">NAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS                                                            </span><br><span class=\"line\">loop0          7:0    0    50G  0 loop                                                                        </span><br><span class=\"line\">sda            8:0    0 476.9G  0 disk                                                                        </span><br><span class=\"line\">├─sda1         8:1    0     8G  0 part                                                                        </span><br><span class=\"line\">├─sda2         8:2    0     2G  0 part                                                                        </span><br><span class=\"line\">├─sda3         8:3    0     1K  0 part                                                                        </span><br><span class=\"line\">└─sda5         8:5    0 466.7G  0 part                                                                        </span><br><span class=\"line\">sdb            8:16   0 119.2G  0 disk                                                                        </span><br><span class=\"line\">├─sdb1         8:17   0  1007K  0 part                                                                        </span><br><span class=\"line\">├─sdb2         8:18   0     1G  0 part /boot/efi                                                              </span><br><span class=\"line\">└─sdb3         8:19   0 118.2G  0 part                                                                        </span><br><span class=\"line\">  ├─pve-swap 253:0    0     8G  0 lvm  [SWAP]                                                                 </span><br><span class=\"line\">  └─pve-root 253:1    0 110.2G  0 lvm  / </span><br></pre></td></tr></table></figure>\n\n<p>您可以通过清理占用磁盘空间较大的文件或目录、扩容磁盘或新购磁盘等几种方式来解决磁盘分区空间使用率达到100%的问题。具体操作步骤如下：</p>\n<p>没办法，杀掉Windows</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\">scp root@192.168.6.2:/var/lib/vz/images/100/vm-100-disk-0.qcow2 .</span>      </span><br><span class=\"line\">root@192.168.6.2&#x27;s password:                                                                                  </span><br><span class=\"line\">vm-100-disk-0.qcow2                                                         100%   50GB  45.1MB/s   18:55 </span><br></pre></td></tr></table></figure>\n\n<p>非正常地退出Matomo和yourls，导致数据库启动出错：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40+00:00 [Note] [Entrypoint]: Switching to dedicated user &#x27;mysql&#x27;               </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40+00:00 [Note] [Entrypoint]: Entrypoint script for MariaDB Server 1:11.2.2+mari</span><br><span class=\"line\">a~ubu2204 started.                                                                                            </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40+00:00 [Note] [Entrypoint]: Initializing database files                       </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40 0 [Warning] Can&#x27;t create test file &#x27;/var/lib/mysql/6be765c4a185.lower-test&#x27; (</span><br><span class=\"line\">Errcode: 13 &quot;Permission denied&quot;)                                                                              </span><br><span class=\"line\">Matomo-DB  | /usr/sbin/mariadbd: Can&#x27;t change dir to &#x27;/var/lib/mysql/&#x27; (Errcode: 13 &quot;Permission denied&quot;)      </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 10:46:40 0 [ERROR] Aborting                                                           </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | Installation of system tables failed!  Examine the logs in                                       </span><br><span class=\"line\">Matomo-DB  | /var/lib/mysql/ for more information.                                                            </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | The problem could be conflicting information in an external                                      </span><br><span class=\"line\">Matomo-DB  | my.cnf files. You can ignore these by doing:                                                     </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  |     shell&gt; /usr/bin/mariadb-install-db --defaults-file=~/.my.cnf                                 </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | You can also try to start the mariadbd daemon with:                                              </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  |     shell&gt; /usr/sbin/mariadbd --skip-grant-tables --general-log &amp;                                </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | and use the command line tool /usr/bin/mariadb                                                   </span><br><span class=\"line\">Matomo-DB  | to connect to the mysql database and look at the grant tables:                                   </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  |     shell&gt; /usr/bin/mariadb -u root mysql                                                        </span><br><span class=\"line\">Matomo-DB  |     MariaDB&gt; show tables;                                                                        </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | Try &#x27;/usr/sbin/mariadbd --help&#x27; if you have problems with paths.  Using                          </span><br><span class=\"line\">Matomo-DB  | --general-log gives you a log in /var/lib/mysql/ that may be helpful.                            </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | The latest information about mariadb-install-db is available at                                  </span><br><span class=\"line\">Matomo-DB  | https://mariadb.com/kb/en/installing-system-tables-mysql_install_db                              </span><br><span class=\"line\">Matomo-DB  | You can find the latest source at https://downloads.mariadb.org and                              </span><br><span class=\"line\">Matomo-DB  | the maria-discuss email list at https://launchpad.net/~maria-discuss                             </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB  | Please check all of the above before submitting a bug report                                     </span><br><span class=\"line\">Matomo-DB  | at https://mariadb.org/jira                                                                      </span><br><span class=\"line\">Matomo-DB  |                                                                                                  </span><br><span class=\"line\">Matomo-DB exited with code 1             </span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2023-12-29  2:43:11 0 [Note] InnoDB: IO Error: 5during write of 16384 bytes, for file ./matomodb/matomo_log_visit.ibd(16), returned 0</span><br></pre></td></tr></table></figure>\n\n\n\n<p>​\t</p>\n<h3 id=\"Docker-运行-Mysql-容器后报错：-ERROR-–initialize-specified-but-the-data-directory-has-files-in-it-Abort\"><a href=\"#Docker-运行-Mysql-容器后报错：-ERROR-–initialize-specified-but-the-data-directory-has-files-in-it-Abort\" class=\"headerlink\" title=\"Docker - 运行 Mysql 容器后报错：[ERROR] –initialize specified but the data directory has files in it. Abort\"></a>Docker - 运行 Mysql 容器后报错：[ERROR] –initialize specified but the data directory has files in it. Abort</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2023-12-29 11:14:38+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.44-1.el7 started.</span><br><span class=\"line\">2023-12-29 11:14:39+00:00 [Note] [Entrypoint]: Switching to dedicated user &#x27;mysql&#x27;</span><br><span class=\"line\">2023-12-29 11:14:39+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.44-1.el7 started.</span><br><span class=\"line\">2023-12-29 11:14:39+00:00 [Note] [Entrypoint]: Initializing database files</span><br><span class=\"line\">2023-12-29T11:14:39.612526Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).</span><br><span class=\"line\">2023-12-29T11:14:39.615112Z 0 [ERROR] --initialize specified but the data directory has files in it. Aborting.</span><br><span class=\"line\">2023-12-29T11:14:39.615174Z 0 [ERROR] Aborting</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"拯救MaraDB；\"><a href=\"#拯救MaraDB；\" class=\"headerlink\" title=\"拯救MaraDB；\"></a>拯救MaraDB；</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">2023-12-29 13:00:07+00:00 [Note] [Entrypoint]: Switching to dedicated user &#x27;mysql&#x27;                                   </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 13:00:07+00:00 [Note] [Entrypoint]: Entrypoint script for MariaDB Server 1:11.2.2+maria~ubu2204 started.  </span><br><span class=\"line\">Matomo-DB  | 2023-12-29 13:00:07+00:00 [Note] [Entrypoint]: Initializing database files                  </span><br></pre></td></tr></table></figure>\n\n<p>原来是docker-compose 运行maradb需要的权限 给mysql用户分配 执行权限！！</p>\n<p>成功启动maradb！！next：无法运行matomo程序<br>        You don’t have permission to access this resource.Server unable to read htaccess file, denying access to be safe</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Matomo     | [Fri Dec 29 21:28:33.351828 2023] [core:crit] [pid 53] (13)Permission denied: [client 172.21.0.1:5230] AH00529: /var/</span><br><span class=\"line\">www/html/.htaccess pcfg_openfile: unable to check htaccess file, ensure it is readable and that &#x27;/var/www/html/&#x27; is executable, re</span><br><span class=\"line\">ferer: https://query.carlzeng.top:3/</span><br></pre></td></tr></table></figure>\n\n<p>unable to check htaccess file, ensure it is readable and that ‘&#x2F;var&#x2F;www&#x2F;html&#x2F;‘ is executable</p>\n<p>给予777的文件夹权限：matomo</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/07/11/668f863ae7758.png\" alt=\"image-20240711151356442\"></p>\n<p>Maradb修复成功</p>\n<p>Mysql重建成功，丢失数据：lost data: 12.3 - 12.28 之间的数据不知道怎么找回来。。。</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/07/11/668f865810e55.png\" alt=\"image-20240711151426311\"></p>\n<p>修复权限以后显示：</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/07/11/668f86b2d0382.png\" alt=\"image-20240711151556999\"></p>\n<p>到目录&#x2F;www&#x2F;server&#x2F;panel&#x2F;data&#x2F;compose&#x2F;yourls_service&#x2F;template下运行 重新运行 docker-compose up -d</p>\n<p>在PVE中发现磁盘没有被正常加载，这时需要</p>\n<p>mount &#x2F;dev&#x2F;sdd2 &#x2F;mnt&#x2F;usbToshiBa</p>\n<h2 id=\"PVE8-0如何配置和限制日志生成\"><a href=\"#PVE8-0如何配置和限制日志生成\" class=\"headerlink\" title=\"PVE8.0如何配置和限制日志生成\"></a>PVE8.0如何配置和限制日志生成</h2><p>You can limit it by editing “#SystemMaxUse&#x3D;” to something like “SystemMaxUse&#x3D;100M” in “&#x2F;etc&#x2F;systemd&#x2F;journald.conf” and then do a:</p>\n<p> systemctl restart systemd-journald</p>\n<p>下一步就是清理Debian系统的磁盘占用</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@lgkdz:/var/lib/vz# du -s /var/lib/vz/images/* | sort -nr                                                   </span><br><span class=\"line\">51647072        /var/lib/vz/images/101                                                                          </span><br><span class=\"line\">25876720        /var/lib/vz/images/105   </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Debian-清理apt缓存文件\"><a href=\"#Debian-清理apt缓存文件\" class=\"headerlink\" title=\"Debian 清理apt缓存文件\"></a>Debian 清理apt缓存文件</h2><h3 id=\"How-to-clear-the-APT-cache-and-delete-everything-from-var-cache-apt-archives\"><a href=\"#How-to-clear-the-APT-cache-and-delete-everything-from-var-cache-apt-archives\" class=\"headerlink\" title=\"How to clear the APT cache and delete everything from &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;\"></a>How to clear the APT cache and delete everything from &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;</h3><p>The clean command clears out the local repository of retrieved package files. It removes everything but the lock file from &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F; and &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;partial&#x2F;. The syntax is:<br><code>sudo apt clean</code><br>OR<br><code>sudo apt-get clean</code></p>\n<h3 id=\"Delete-all-useless-files-from-the-APT-cache\"><a href=\"#Delete-all-useless-files-from-the-APT-cache\" class=\"headerlink\" title=\"Delete all useless files from the APT cache\"></a>Delete all useless files from the APT cache</h3><p>The syntax is as follows to delete &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;:<br><code>sudo apt autoclean</code><br>OR<br><code>sudo apt-get autoclean</code></p>\n<p>docker system prune</p>\n<h2 id=\"如何设置docker每天定时重启\"><a href=\"#如何设置docker每天定时重启\" class=\"headerlink\" title=\"如何设置docker每天定时重启\"></a>如何设置docker每天定时重启</h2><p>NAS中耗磁盘的docker设置重启</p>\n<p>echo “0 3 * * * root docker restart embyserver” &gt;&gt; &#x2F;etc&#x2F;cron.d&#x2F;my-cron</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;0 3 * * * root docker restart embyserver&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;11 3 * * * root docker restart portainer&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;16 3 * * * root docker restart it-tools&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;21 3 * * * root docker restart espeakbox&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;26 3 * * * root docker restart book-searcher&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;31 3 * * * root docker restart aria2-webui&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;36 3 * * * root docker restart alistaria2-webui&quot; &gt;&gt; /etc/cron.d/my-cron</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>20230109 添加了更多的重启</p>\n<p>继续观察，是否按时重启了这些docker容器？</p>\n<p>答：确实已经是每天定时重启了</p>\n</blockquote>\n<h2 id=\"Debian中耗磁盘的docker设置重启\"><a href=\"#Debian中耗磁盘的docker设置重启\" class=\"headerlink\" title=\"Debian中耗磁盘的docker设置重启\"></a>Debian中耗磁盘的docker设置重启</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">At 02:00am, only on Monday</span></span><br><span class=\"line\">echo &quot;0 2 * * 1 root docker restart iptvchecker-website-1&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;0 3 * * * root docker restart frps&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;10 3 * * * root docker restart chatgpt-next-web&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;15 3 * * * root docker restart hideipnetwork&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;20 3 * * * root docker restart mmPlayer&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\">echo &quot;25 3 * * * root docker restart hysteria&quot; &gt;&gt; /etc/cron.d/my-cron</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<p>如何改变 &#x2F;dev&#x2F;sda5的大小？</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@lgkdz:~# df -hT                                                                                </span><br><span class=\"line\">Filesystem           Type      Size  Used Avail Use% Mounted on                                     </span><br><span class=\"line\">udev                 devtmpfs  7.7G     0  7.7G   0% /dev                                           </span><br><span class=\"line\">tmpfs                tmpfs     1.6G  880K  1.6G   1% /run                                           </span><br><span class=\"line\">/dev/mapper/pve-root ext4      109G   85G   19G  82% /                                              </span><br><span class=\"line\">tmpfs                tmpfs     7.8G   46M  7.7G   1% /dev/shm                                       </span><br><span class=\"line\">tmpfs                tmpfs     5.0M     0  5.0M   0% /run/lock                                      </span><br><span class=\"line\">/dev/sdb2            vfat     1022M  352K 1022M   1% /boot/efi                                      </span><br><span class=\"line\">/dev/fuse            fuse      128M   16K  128M   1% /etc/pve                                       </span><br><span class=\"line\">tmpfs                tmpfs     1.6G     0  1.6G   0% /run/user/0   </span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"PVE挂载新硬盘\"><a href=\"#PVE挂载新硬盘\" class=\"headerlink\" title=\"PVE挂载新硬盘\"></a>PVE挂载新硬盘</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ls /dev/disk/by-id</span><br><span class=\"line\"></span><br><span class=\"line\">qm set 105 -sata2 /dev/disk/by-id/ata-Kingchuxing_512GB_2023050801426</span><br><span class=\"line\">qm set 105 -sata3 /dev/disk/by-id/usb-TOSHIBA_External_USB_3.0_20140612002491C-0:0</span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">这是挂载到特定的虚拟机中去的命令；</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">我需要挂载给pve本身！</span></span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fdisk /dev/sdc   </span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">格式化</span><br><span class=\"line\">mkfs.ext4 /dev/sdc2</span><br><span class=\"line\"></span><br><span class=\"line\">/dev/sdc2 contains a hfsplus file system labelled &#x27;Time Machine Backups&#x27;</span><br><span class=\"line\">Proceed anyway? (y,N) y</span><br><span class=\"line\">Creating filesystem with 244106668 4k blocks and 61030400 inodes</span><br><span class=\"line\">Filesystem UUID: 3991ce4f-dcec-4b28-9370-3ebb751c1912</span><br><span class=\"line\">Superblock backups stored on blocks: </span><br><span class=\"line\">        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, </span><br><span class=\"line\">        4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, </span><br><span class=\"line\">        102400000, 214990848</span><br><span class=\"line\"></span><br><span class=\"line\">Allocating group tables: done                            </span><br><span class=\"line\">Writing inode tables: done                            </span><br><span class=\"line\">Creating journal (262144 blocks): </span><br><span class=\"line\">done</span><br><span class=\"line\">Writing superblocks and filesystem accounting information: done     </span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mount /dev/sdc2 /mnt/usbToshiBa1T</span><br></pre></td></tr></table></figure>\n\n<p>等待测试PVE的显卡直通情况，确保，开机的画面是pve的，这样如果这个命令是错误的（导致无法开机）</p>\n<p>echo &#x2F;dev&#x2F;sdc2 &#x2F;mnt&#x2F;usbToshiBa1T ext4 defaults 0 0 &gt;&gt; &#x2F;etc&#x2F;fstab</p>\n<p>就还知道补救的步骤</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如果这里你操作错误，可能会导致 PVE 无法启动，需要在启动时候接上显示器，进入修复模式 repair filesystem ，直接输入 root 密码即可进入</span><br><span class=\"line\"></span><br><span class=\"line\">因为此时 / 目录是只读模式，进行修改 /etc/fstab 时，提示无法保存（只读），这时需要将 / 目录重新挂载为可读写模式 ，用命令</span><br><span class=\"line\"></span><br><span class=\"line\">mount -o remount,rw,auto /</span><br><span class=\"line\">然后再对 /etc/fstab 进行修改就可以了。重启后系统正常启动。</span><br><span class=\"line\"></span><br><span class=\"line\">之后重启 PVE 即可</span><br><span class=\"line\"></span><br><span class=\"line\">reboot</span><br></pre></td></tr></table></figure>\n\n\n\n<p>这个可以看到已经mount成功了，新磁盘可以使用了</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt; </span><span class=\"language-bash\"><span class=\"built_in\">df</span> -hT</span>                                                                  </span><br><span class=\"line\">Filesystem           Type      Size  Used Avail Use% Mounted on                                       </span><br><span class=\"line\">udev                 devtmpfs  7.7G     0  7.7G   0% /dev                                             </span><br><span class=\"line\">tmpfs                tmpfs     1.6G  904K  1.6G   1% /run                                             </span><br><span class=\"line\">/dev/mapper/pve-root ext4      109G   85G   19G  82% /                                                </span><br><span class=\"line\">tmpfs                tmpfs     7.8G   46M  7.7G   1% /dev/shm                                         </span><br><span class=\"line\">tmpfs                tmpfs     5.0M     0  5.0M   0% /run/lock                                        </span><br><span class=\"line\">/dev/sdb2            vfat     1022M  352K 1022M   1% /boot/efi                                        </span><br><span class=\"line\">/dev/fuse            fuse      128M   16K  128M   1% /etc/pve                                         </span><br><span class=\"line\">tmpfs                tmpfs     1.6G     0  1.6G   0% /run/user/0                                      </span><br><span class=\"line\">/dev/sdc2            ext4      916G   12K  869G   1% /mnt/usbToshiBa1T    </span><br></pre></td></tr></table></figure>\n\n\n\n<p>本章节参考：</p>\n<p><a href=\"https://www.moewah.com/archives/2546.html\">Proxmox VE（PVE）如何添加多块硬盘</a></p>\n<p><a href=\"https://www.moewah.com/archives/2558.html\">Linux&#x2F;VPS开机启动自动挂载分区的方法</a></p>\n<p><a href=\"https://anuoua.github.io/2019/03/04/Linux%E6%97%A0%E6%8D%9F%E8%B0%83%E6%95%B4%E5%88%86%E5%8C%BA%E5%A4%A7%E5%B0%8F/\">Linux无损调整分区大小</a></p>\n<p><a href=\"https://www.lategege.com/?p=538\">Proxmox VE(PVE)添加硬盘详解</a></p>\n<p>第二天，sdc消失，变成了sdd.</p>\n<p>直接重新拔插USB，还是识别成sdd。删除昨天的sdc2 mount.</p>\n<p>使用pve的UI，点击磁盘》&#x2F;dev&#x2F;sdd2，点 ‘擦除磁盘’，使用率由昨天格式化的ext4 变成了 否。</p>\n<p>umount &#x2F;mnt&#x2F;usbToshiBa1T</p>\n<p>在UI中（目录：磁盘》LVM），新建Volume Group。</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f40b956487.png\" alt=\"image-20240417111839114\"></p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f40bcd9848.png\" alt=\"image-20240417111903877\"></p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f40b088708.png\" alt=\"image-20240417112326350\"></p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f42778ca3e.png\" alt=\"image-20240417113101380\"></p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/17/661f43ddd8091.png\" alt=\"image-20240417113659986\"></p>\n<p>top, ps查了半天，还是在pve里面的重启指令生效了 :-)</p>\n<h2 id=\"群晖中清理新硬盘容量\"><a href=\"#群晖中清理新硬盘容量\" class=\"headerlink\" title=\"群晖中清理新硬盘容量\"></a>群晖中清理新硬盘容量</h2><p>日常使用中有手动的把不需要的文件都删除，可是依然发现磁盘的使用率一路表现直到75% 然后 82%（报警）。</p>\n<p>今天发现（其实以前清理过），需要群晖中删除的文件都会默认被挪动到‘回收站’；直到我手动地在该硬盘的根目录，删除‘#recovery’文件夹。磁盘使用率恢复成36%，高兴了 :-)</p>\n<p><img data-src=\"https://img.carlzeng.top:3/i/2024/04/22/66264a39070f7.png\" alt=\"image-20240422192957075\"></p>\n<h2 id=\"分配PVE新的USB磁盘给Debian\"><a href=\"#分配PVE新的USB磁盘给Debian\" class=\"headerlink\" title=\"分配PVE新的USB磁盘给Debian\"></a>分配PVE新的USB磁盘给Debian</h2><p>Debian是以docker的形式挂载在PVE上的，点击pve UI中的Debian11 &gt; ‘资源’ &gt; 添加‘挂载点’</p>\n<p>分配了USB ToShiBa 的硬盘中的 400G给Debian。</p>\n<p>然后备份Debian和Openwrt到这个USB ToShiBa 的硬盘400G空间中，然后都结束以后，PVE又显示磁盘的状态：unknown</p>\n<p>点击磁盘后选择‘备份’显示错误：</p>\n<p>mkdir &#x2F;mnt&#x2F;usbToshiBa&#x2F;&#x2F;dump: Input&#x2F;output error at &#x2F;usr&#x2F;share&#x2F;perl5&#x2F;PVE&#x2F;Storage&#x2F;Plugin.pm line 1389. (500)</p>\n<p>重新拔插了USB的ToshiBa移动硬盘；依然显示状态：unknown</p>\n<p>修复办法：</p>\n<p>​\t切换到PVE的Shell执行</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mount /dev/sdc2 /mnt/usbToshiBa</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">#</span><span class=\"language-bash\">20240423/CZ 尝试通过修改/etc/fstab来获取自动加载，不知道下次会不会自动加载？</span></span><br><span class=\"line\">echo /dev/sdc2 /mnt/usbToshiBa ext4 defaults 0 0 &gt;&gt; /etc/fstab</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"相关内容\"><a href=\"#相关内容\" class=\"headerlink\" title=\"相关内容\"></a>相关内容</h1><iframe style=\"box-shadow: 0px 0px 20px -10px;\" src=\"https://query.carlzeng.top:3/appsearch?q=nas\" frameborder=\"0\" scrolling=\"auto\" width=\"100%\" height=\"500\"></iframe>\n\n<h1 id=\"实现方法\"><a href=\"#实现方法\" class=\"headerlink\" title=\"实现方法\"></a>实现方法</h1><p>都是些小爱心小公益的项目，没必要花时间攻击哦，沉淀共享，格局放大！～</p>","categories":[{"name":"NAS","path":"api/categories/NAS.json"}],"tags":[{"name":"docker-compose","path":"api/tags/docker-compose.json"},{"name":"docker","path":"api/tags/docker.json"},{"name":"家宽","path":"api/tags/家宽.json"},{"name":"NAS","path":"api/tags/NAS.json"},{"name":"linux","path":"api/tags/linux.json"},{"name":"pve","path":"api/tags/pve.json"}]}